Query 1: Courses filtered to ratings over 4.4, select specific fields and include the instructor facet.  
{
  "filter": "rating_average gt 4.4",
  "select": "description, rating_average,instructor",
  "facets": ["instructor"]
}

Response 1:
{
  "@odata.context": "https://training-catalog-jms.search.windows.net/indexes('azuretable-index')/$metadata#docs(*)",
  "@search.facets": {
    "instructor": [
      {
        "count": 3,
        "value": "Mike Montoya"
      },
      {
        "count": 2,
        "value": "Claudia Blackman"
      },
      {
        "count": 2,
        "value": "Gerald Dominguez"
      },
      {
        "count": 1,
        "value": "Eileen Diaz"
      }
    ]
  },
  "@search.nextPageParameters": {
    "facets": [
      "instructor"
    ],
    "select": "description, rating_average,instructor",
    "filter": "rating_average gt 4.4",
    "skip": 50
  },
  "value": [
    {
      "@search.score": 1,
      "description": "Learn our internal best practices for using the O365 suite including email signatures, file storage and other issues",
      "instructor": "Gerald Dominguez",
      "rating_average": 4.6
    },
    {
      "@search.score": 1,
      "description": "For developers, learn our best practices for securely connecting to databases",
      "instructor": "Eileen Diaz",
      "rating_average": 4.8
    },
    {
      "@search.score": 1,
      "description": "Learn the policies related to the distribution and use of computers, phones, software, and other technology",
      "instructor": "Mike Montoya",
      "rating_average": 4.9
    },
    {
      "@search.score": 1,
      "description": "For administrators, this course will teach you how our CI/CD pipelines work from an operations perspective",
      "instructor": "Claudia Blackman",
      "rating_average": 4.9
    },
    {
      "@search.score": 1,
      "description": "Learn how to track billable and non-billable hours by assigning time to projects and other relevant time codes",
      "instructor": "Mike Montoya",
      "rating_average": 4.8
    },
    {
      "@search.score": 1,
      "description": "This course will teach you the specific ways our company uses Git. You will learn details for comments, branching, pull requests, and other procsses",
      "instructor": "Claudia Blackman",
      "rating_average": 4.5
    },
    {
      "@search.score": 1,
      "description": "This course will teach you best practices for communicating with your team while working remotely",
      "instructor": "Gerald Dominguez",
      "rating_average": 4.7
    },
    {
      "@search.score": 1,
      "description": "Understand ways you can be more healthy in the work environment including what ergonomic equipment is available to you",
      "instructor": "Mike Montoya",
      "rating_average": 4.6
    },
    {
      "@search.score": 1,
      "description": "Find out about automated testing that proves your code to be maintainable, understandable, and functioning without repetitive manual testing.",
      "instructor": null,
      "rating_average": 4.73
    },
    {
      "@search.score": 1,
      "description": "Enable business users with key AI use cases",
      "instructor": null,
      "rating_average": 4.75
    },
    {
      "@search.score": 1,
      "description": "Explore the strategic components, use cases, and special factors of an enterprise AI strategy that creates real business value, with INSEAD and Microsoft.",
      "instructor": null,
      "rating_average": 4.71
    },
    {
      "@search.score": 1,
      "description": "Learn to manage LUIS apps through versioning, key management, handling data, and improving predictions.",
      "instructor": null,
      "rating_average": 4.77
    },
    {
      "@search.score": 1,
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "instructor": null,
      "rating_average": 4.75
    },
    {
      "@search.score": 1,
      "description": "Learn about AI Builder Text recognition and how to use it with other Power Platform products.",
      "instructor": null,
      "rating_average": 4.61
    },
    {
      "@search.score": 1,
      "description": "In this module, we'll introduce you to Language Understanding Intelligent Service (LUIS) and show how to build and publish a LUIS model",
      "instructor": null,
      "rating_average": 4.67
    },
    {
      "@search.score": 1,
      "description": "Learn to identify valuable information in conversations with LUIS for interpreting user goals (intents) and distill valuable information from sentences (entities).",
      "instructor": null,
      "rating_average": 4.77
    },
    {
      "@search.score": 1,
      "description": "Explore two capabilities in the DevOps taxonomy, Continuous Collaboration and Continuous Improvement.",
      "instructor": null,
      "rating_average": 4.7
    },
    {
      "@search.score": 1,
      "description": "Learn to identify valuable information in conversations with LUIS for interpreting user goals (intents) and distill valuable information from sentences (entities).",
      "instructor": null,
      "rating_average": 4.77
    },
    {
      "@search.score": 1,
      "description": "Implement CI/CD pipeline for Node.js applications. Automate deployment of Vue, React, Angular or webpack apps with Azure Pipelines.",
      "instructor": null,
      "rating_average": 4.9
    },
    {
      "@search.score": 1,
      "description": "Choose and implement a deployment pattern that helps you smoothly roll out new application features to your users.",
      "instructor": null,
      "rating_average": 4.72
    },
    {
      "@search.score": 1,
      "description": "Learn how to manage changes to your repository source by using pull requests.",
      "instructor": null,
      "rating_average": 4.75
    },
    {
      "@search.score": 1,
      "description": "Learn how to contribute to an open-source project on GitHub.",
      "instructor": null,
      "rating_average": 4.88
    },
    {
      "@search.score": 1,
      "description": "Implement robotic process automation with Microsoft Power Automate, Teams, desktop flow, and AI Builder",
      "instructor": null,
      "rating_average": 4.73
    },
    {
      "@search.score": 1,
      "description": "In this module, we'll authenticate to GitHub, create a local Git repository, and push the repository to GitHub by using the Git tooling experience in Visual Studio 2019. We'll add and modify files, stage and commit changes, and then finally push to your remote.",
      "instructor": null,
      "rating_average": 5
    },
    {
      "@search.score": 1,
      "description": "Get started with AI on Azure",
      "instructor": null,
      "rating_average": 4.78
    },
    {
      "@search.score": 1,
      "description": "Learn to use the GitHub integration in Visual Studio Code, including authentication, publishing repos, and viewing your repo timeline.",
      "instructor": null,
      "rating_average": 4.82
    },
    {
      "@search.score": 1,
      "description": "Use Python, Flask, and Azure Cognitive Services to build a web app that incorporates AI",
      "instructor": null,
      "rating_average": 4.65
    },
    {
      "@search.score": 1,
      "description": "Implement a CI/CD pipeline for multiple containers to Kubernetes.",
      "instructor": null,
      "rating_average": 4.7
    },
    {
      "@search.score": 1,
      "description": "DevOps is the union of people, process, and products to enable continuous delivery of value to our end users. Discover the first two foundation pillars of DevOps: Culture and Lean Product.",
      "instructor": null,
      "rating_average": 4.69
    },
    {
      "@search.score": 1,
      "description": "Run Selenium UI tests, a form of functional testing, in Azure Pipelines.",
      "instructor": null,
      "rating_average": 4.66
    },
    {
      "@search.score": 1,
      "description": "Implement a CI/CD pipeline for Python.",
      "instructor": null,
      "rating_average": 4.78
    },
    {
      "@search.score": 1,
      "description": "Learn to manage a successful InnerSource program on GitHub through effective discoverability, guidance, and maintenance.",
      "instructor": null,
      "rating_average": 4.75
    },
    {
      "@search.score": 1,
      "description": "Understand the key elements that make a culture AI-ready and a framework to drive the change needed in your organization for a successful AI strategy.",
      "instructor": null,
      "rating_average": 4.74
    },
    {
      "@search.score": 1,
      "description": "Analyze images with the Computer Vision service",
      "instructor": null,
      "rating_average": 4.73
    },
    {
      "@search.score": 1,
      "description": "Explore two capabilities in the DevOps taxonomy, Continuous Collaboration and Continuous Improvement.",
      "instructor": null,
      "rating_average": 4.7
    },
    {
      "@search.score": 1,
      "description": "Do you want to know how to configure and set up an automated build process? In this module, you'll learn how to configure CI/CD with Business Central applications, how to create pipelines using YAML files, and set up a release process.",
      "instructor": null,
      "rating_average": 4.62
    },
    {
      "@search.score": 1,
      "description": "Create a basic GitHub Action and use that action in a workflow.",
      "instructor": null,
      "rating_average": 4.71
    },
    {
      "@search.score": 1,
      "description": "Detect and track polar bears through photos using AI, and then use Power BI to show where cameras spot polar bears.",
      "instructor": null,
      "rating_average": 4.68
    },
    {
      "@search.score": 1,
      "description": "Learn how to contribute to an open-source project on GitHub.",
      "instructor": null,
      "rating_average": 4.88
    },
    {
      "@search.score": 1,
      "description": "Publish automatically and securely your code libraries or Docker images with GitHub Packages.",
      "instructor": null,
      "rating_average": 4.71
    },
    {
      "@search.score": 1,
      "description": "Use a release approval in Azure Pipelines to help coordinate database schema changes between developers and database administrators.",
      "instructor": null,
      "rating_average": 4.65
    },
    {
      "@search.score": 1,
      "description": "Explore the first two capabilities in the DevOps taxonomy, Continuous Planning and Continuous Integration.",
      "instructor": null,
      "rating_average": 4.73
    },
    {
      "@search.score": 1,
      "description": "Explore how to use GitHub Actions to create an automated Azure Kubernetes Service deployment pipeline.",
      "instructor": null,
      "rating_average": 4.5
    },
    {
      "@search.score": 1,
      "description": "Learn Microsoft guidelines for the development of responsible conversational AI, such as chat bots and voice-controlled systems.",
      "instructor": null,
      "rating_average": 4.82
    },
    {
      "@search.score": 1,
      "description": "Learn how infrastructure as code enables you to describe and automatically provision the infrastructure that you need for your application.",
      "instructor": null,
      "rating_average": 4.69
    },
    {
      "@search.score": 1,
      "description": "Azure Pipelines help automate building, deploying, and maintaining your applications. While they support a wide range of platforms and programming languages, in this module you’ll focus on using them to implement ASP.NET apps on Azure App Service Web Apps with Azure SQL Database as their data store.",
      "instructor": null,
      "rating_average": 4.85
    },
    {
      "@search.score": 1,
      "description": "Run automated load tests by using Apache JMeter, a form of nonfunctional testing, in Azure Pipelines.",
      "instructor": null,
      "rating_average": 4.71
    },
    {
      "@search.score": 1,
      "description": "Create a language translator application for your mixed reality device (Windows Mixed Reality, HoloLens 2, etc.) by using Cognitive Services.",
      "instructor": null,
      "rating_average": 4.6
    },
    {
      "@search.score": 1,
      "description": "Learn enterprise AI management with our free open online course, including machine learning.",
      "instructor": null,
      "rating_average": 4.75
    },
    {
      "@search.score": 1,
      "description": "Implement GitHub Actions to build a container image and deploy to Azure Kubernetes Service.",
      "instructor": null,
      "rating_average": 4.79
    }
  ],
  "@odata.nextLink": "https://training-catalog-jms.search.windows.net/indexes/azuretable-index/docs/search?api-version=2024-05-01-preview"
}


Query 2: Filter Library data to specific author.  
{
  "search": "*",
  "filter": "metadata_author eq 'Alina Köchling '"
}

Response 2: 
{
  "@odata.context": "https://training-catalog-jms.search.windows.net/indexes('azureblob-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 1,
      "content": "\nORIGINAL RESEARCH\n\nDiscriminated by an algorithm: a systematic review\nof discrimination and fairness by algorithmic decision-\nmaking in the context of HR recruitment and HR\ndevelopment\n\nAlina Köchling1\n• Marius Claus Wehner1\n\nReceived: 15 October 2019 / Accepted: 1 November 2020 / Published online: 20 November 2020\n\n� The Author(s) 2020\n\nAbstract Algorithmic decision-making is becoming increasingly common as a new\n\nsource of advice in HR recruitment and HR development. While firms implement\n\nalgorithmic decision-making to save costs as well as increase efficiency and\n\nobjectivity, algorithmic decision-making might also lead to the unfair treatment of\n\ncertain groups of people, implicit discrimination, and perceived unfairness. Current\n\nknowledge about the threats of unfairness and (implicit) discrimination by algo-\n\nrithmic decision-making is mostly unexplored in the human resource management\n\ncontext. Our goal is to clarify the current state of research related to HR recruitment\n\nand HR development, identify research gaps, and provide crucial future research\n\ndirections. Based on a systematic review of 36 journal articles from 2014 to 2020,\n\nwe present some applications of algorithmic decision-making and evaluate the\n\npossible pitfalls in these two essential HR functions. In doing this, we inform\n\nresearchers and practitioners, offer important theoretical and practical implications,\n\nand suggest fruitful avenues for future research.\n\nKeywords Fairness � Discrimination � Perceived fairness � Ethics �\nAlgorithmic decision-making in HRM � Literature review\n\n1 Introduction\n\nAlgorithmic decision-making in human resource management (HRM) is becoming\n\nincreasingly common as a new source of information and advice, and it will gain\n\nmore importance due to the rapid growth of digitalization in organizations.\n\n& Alina Köchling\n\nalina.koechling@hhu.de\n\n1 Faculty of Business Administration and Economics, Heinrich-Heine-University Düsseldorf,\n\nUniversitätsstrasse 1, 40225 Dusseldorf, Germany\n\n123\n\nBusiness Research (2020) 13:795–848\n\nhttps://doi.org/10.1007/s40685-020-00134-w\n\nhttp://orcid.org/0000-0001-7039-9852\nhttp://orcid.org/0000-0002-1932-3155\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40685-020-00134-w&amp;domain=pdf\nhttps://doi.org/10.1007/s40685-020-00134-w\n\n\nAlgorithmic decision-making is defined as automated decision-making and remote\n\ncontrol, as well as standardization of routinized workplace decisions (Möhlmann\n\nand Zalmanson 2017). Algorithms, instead of humans, make decisions, and this has\n\nimportant individual and societal implications in organizational optimization\n\n(Chalfin et al. 2016; Lee 2018; Lindebaum et al. 2019). These changes in favor\n\nof algorithmic decision-making make it easier to discover hidden talented\n\nemployees in organizations and review a large number of applications automatically\n\n(Silverman and Waller 2015; Carey and Smith 2016; Savage and Bales 2017). In a\n\nsurvey of 200 artificial intelligence (AI) specialists from German companies, 79%\n\nstated that AI is irreplaceable for competitive advantages (Deloitte 2020). Several\n\ncommercial providers, such as Google, IBM, SAP, and Microsoft, already offer\n\nalgorithmic platforms and systems that facilitate current human resource (HR)\n\npractices, such as hiring and performance measurements (Walker 2012). In turn,\n\nwell-known and large companies, such as Vodafone, Intel, Unilever, and Ikea, apply\n\nalgorithmic decision-making in HR recruitment and HR development (Daugherty\n\nand Wilson 2018; Precire 2020).\n\nThe major driving forces for algorithmic decision-making are savings in both\n\ncosts and time, minimizing risks, enhancing productivity, and increasing certainty in\n\ndecision-making (Suen et al. 2019; McDonald et al. 2017; McColl and Michelotti\n\n2019; Woods et al. 2020). Besides these economic reasons, firms seek to diminish\n\nthe human biases (e.g., prejudices and personal beliefs) by applying algorithmic\n\ndecision-making, thereby increasing the objectivity, consistency, and fairness of the\n\nHR recruitment as well as HR development processes (Langer et al. 2019;\n\nFlorentine 2016; Raghavan et al. 2020). For example, Deloitte argues that the\n\nalgorithmic decision-making system always manages each application with the\n\nsame attention according to the same requirements and criteria (Deloitte 2018). At\n\nfirst glance, algorithmic decision-making seems to be more objective and fairer than\n\nhuman decision-making (Lepri et al. 2018).\n\nHowever, there is a possible threat of discrimination and unfairness by relying\n\nsolely on algorithmic decision-making (e.g., (Lee 2018; Lindebaum et al. 2019;\n\nSimbeck 2019)). In general, discrimination is defined as the unequal treatment of\n\ndifferent groups based on gender, age, or ethnicity instead of on qualitative\n\ndifferences, such as individual performance (Arrow 1973). Algorithms produce\n\ndiscrimination or biased outcomes if they are trained on inaccurate (Kim 2016),\n\nbiased (Barocas and Selbst 2016), or unrepresentative input data (Suresh and Guttag\n\n2019). Consequently, algorithms are vulnerable to produce or replicate biased\n\ndecisions if their input (or training) data are biased (Chander 2016).\n\nComplicating this issue, biases and discrimination are often only recognized after\n\nalgorithms have made a decision. As a prominent example stemming from the\n\ncurrent debate around transparency, bias, and fairness in algorithmic decision-\n\nmaking (Dwork et al. 2012; Lepri et al. 2018; Diakopoulos 2015), the hiring\n\nalgorithms applied by the American e-commerce specialist Amazon yielded an\n\nextreme disadvantage of female applicants, which finally led Amazon to shut down\n\nthe complete algorithmic decision-making for their hiring decision (Dastin 2018;\n\nMiller 2015). Thus, the lack of transparency and accountability of the input data, the\n\nalgorithm itself, and the factors influencing algorithmic outcomes are potential\n\n796 Business Research (2020) 13:795–848\n\n123\n\n\n\nissues associated with algorithmic decision-making (Citron and Pasquale 2014;\n\nPasquale 2015). Another question remains whether applicants and/or employees\n\nperceive the algorithmic decision-making to be fair. Previous studies showed that\n\napplicants’ and employees’ acceptance of algorithmic decision-making is lower in\n\nHR recruitment and HR development compared to common procedures conducted\n\nby humans (Kaibel et al. 2019; Langer et al. 2019; Lee 2018).\n\nConsequently, there is a discrepancy between the enthusiasm about algorithmic\n\ndecision-making as a panacea for inefficiencies and labor shortages on one hand and\n\nthe threat of discrimination and unfairness of algorithmic decision-making on the\n\nother side. While the literature in the field of computer science has already\n\naddressed the issues of biases, knowledge about the potential downsides of\n\nalgorithmic decision-making is still in its infancy in the field of HRM despite its\n\nimportance due to increased digitization and automation in HRM. This heteroge-\n\nneous state of research on discrimination and fairness raises distinct challenges for\n\nfuture research. From a practical point of view, it is problematic if large and well-\n\nknown companies implement algorithms without being aware of the possible pitfalls\n\nand negative consequences. Thus, to move the field forward, it is paramount to\n\nsystematically review and synthesize existing knowledge about biases and\n\ndiscrimination in algorithmic decision-making and to offer new research avenues.\n\nThe aim of this study is threefold. First, this review creates an awareness of\n\npotential biases and discrimination resulting from algorithmic decision-making in\n\nthe context of HR recruitment and HR development. Second, this study contributes\n\nto the current literature by informing both researchers and practitioners about the\n\npotential dangers of algorithmic decision-making in the HRM context. Finally, we\n\nguide future research directions with an understanding of existing knowledge and\n\ngaps in the literature. To this end, the present paper conducts a systematic review of\n\nthe current literature with a focus on HR recruitment and HR development. These\n\ntwo HR functions deal with the potential of future and current employees and the\n\n(automatic) prediction of person-organization fit, career development, and future\n\nperformance (Huselid 1995; Walker 2012). Decisions made by algorithms and AI in\n\nthese two important HR areas have serious consequences for individuals, the\n\ncompany, and society concerning ethics and both procedural and distributive\n\nfairness (Ötting and Maier 2018; Lee 2018; Tambe et al. 2019; Cappelli et al. 2020).\n\nOur study contributes to the existing body of research in several ways. First, the\n\nsystematic literature review contributes to the literature by highlighting the current\n\ndebate on ethical issues associated with algorithmic decision-making, including bias\n\nand discrimination (Barocas and Selbst 2016). Second, our research provides\n\nillustrative examples of various algorithmic decision-making tools used in HR\n\nrecruitment, HR development, and their potential for discrimination and perceived\n\nfairness. Moreover, our systematic review underlines the fact that it is a timely topic\n\ngaining enormous importance. Companies will face legal and reputational risk if\n\ntheir HR recruitment and HR development methods turn out to be discriminatory,\n\nand applicants and employees may consider the algorithmic selection or develop-\n\nment process to be unfair.\n\nFor this reason, companies need to know that the use of algorithmic decision-\n\nmaking can yield to discrimination, unfairness, and dissatisfaction in the context of\n\nBusiness Research (2020) 13:795–848 797\n\n123\n\n\n\nHRM. We offer an understanding of how discrimination might arise when\n\nimplementing algorithmic decision-making. We try to give guidance on how\n\ndiscrimination and perceived unfairness could be avoided and provide detailed\n\ndirections for future research in the existing literature, especially in the HRM field.\n\nMoreover, we identify several research gaps, mainly a lacking focus on perceived\n\nfairness.\n\nThe paper is organized as follows: first, we give an understanding of key terms\n\nand definitions. Afterward, we present the methodology of our systematic literature\n\nreview accompanied by a descriptive analysis of the reviewed literature. This is\n\nfollowed by an illustration of the current state of knowledge on algorithmic\n\ndecision-making and subsequent discussion. Finally, we offer practical as well as\n\ntheoretical implications and outline future research avenues.\n\n2 Conceptual background and definitions\n\n2.1 Definition of algorithms\n\nThe Oxford Living Dictionary defines algorithms as ‘‘processes or sets of rules to be\n\nfollowed in calculations or other problem-solving operations, especially by a\n\ncomputer.’’ Möhlmann and Zalmanson (2017) refer to algorithmic decision-making\n\nas automated decision-making and remote control, and standardization of routinized\n\nworkplace decision. Thus, in this paper, we use the term algorithmic decision-\n\nmaking to describe a computational mechanism that autonomously makes decisions\n\nbased on rules and statistical models without explicit human interference (Lee\n\n2018). Algorithms are the basis for several AI decision tools.\n\nAI is an umbrella term for a wide array of models, methods, and prescriptions\n\nused to simulate human intelligence, often when it comes to collecting, processing,\n\nand acting on data. AI applications can apply rules, learn over time through the\n\nacquisition of new data and information, and adapt to changes in the environment\n\n(Russell and Norvig 2016). AI includes several different research areas, such as\n\nmachine learning (ML), speech and image recognition, and natural language\n\nprocessing (NLP) (Kaplan and Haenlein 2019; Paschen et al. 2020).\n\nAs mentioned, the basis for many AI decision-making tools used in HR are ML\n\nalgorithms, which can be categorized into three major types: supervised, unsuper-\n\nvised, and reinforcement learning (Lee and Shin 2020). Supervised ML algorithms\n\naim to make predictions (often divided into classification- or regression-type\n\nproblems), given the input data and desired outputs considered as the ground truth.\n\nHuman experts often provide these labels and thus provide the algorithm with the\n\nground truth. To replicate human decisions or to make predictions, the algorithm\n\nlearns patterns from the labeled data and develops rules, which can be applied for\n\nfuture instances for the same problem (Canhoto and Clear 2020). In contrast, in\n\nunsupervised ML, only input data are given, and the model learns patterns from the\n\ndata without a priori labeling (Murphy 2012). Unsupervised ML algorithms capture\n\nthe structural behaviors of variables in the input data for theme analysis or grouping\n\n798 Business Research (2020) 13:795–848\n\n123\n\n\n\ndata (Canhoto and Clear 2020). Finally, reinforcement learning, as a separate group\n\nof methods, is not based on fixed input/output data. Instead, the ML algorithm learns\n\nbehavior through trial-and-error interactions with a dynamic environment (Kael-\n\nbling et al. 1996).\n\nFurthermore, instead of grouping ML models as supervised, unsupervised, or\n\nreinforcement type learning, the methodologies of algorithms may also be used to\n\ncategorize ML models. Examples are probabilistic models, which may be used in\n\nsupervised or unsupervised settings (Murphy 2012), or deep learning models (Lee\n\nand Shin 2020), which rely on artificial neural networks and perform complex\n\nlearning tasks. In supervised settings, neural network models often determine the\n\nrelationship between input and output using network structures containing the so-\n\ncalled hidden layers, meaning phases of transformation of the input data. Single\n\nnodes of these layers (neurons) were first modeled after neurons in the human brain,\n\nand they resemble human thinking (Bengio et al. 2017). In other settings, deep\n\nlearning may be used, for instance, to (1) process information through multiple\n\nstages of nonlinear transformation; or (2) determine features, representations of the\n\ndata providing an advantage for, e.g., prediction tasks (Deng and Yu 2014).\n\n2.2 Reason for biases\n\nFor any estimation bY of a random variable Y , bias refers to the difference between\n\nthe expected values of bY and Y and is also referred to as systematic error\n\n(Kauermann and Kuechenhoff 2010; Goodfellow et al. 2016). Cognitive biases,\n\nspecifically, are systematic errors in human judgment when dealing with uncertainty\n\n(Kahneman et al. 1982). These cognitive biases are thought to be transferred to\n\nalgorithmic evaluations or predictions, where bias may refer to ‘‘computer systems\n\nthat systematically and unfairly discriminate against certain individuals or groups in\n\nfavor of others’’ (Friedman and Nissenbaum 1996, p. 332).\n\nAlgorithms are often characterized as ‘‘black box’’. In the context of HRM,\n\nCheng and Hackett (2019) characterize algorithms as ‘‘glass boxes’’, since some,\n\nbut not all, components of the theory are reflective. In this context, the consideration\n\nand distinction of the three core elements are necessary, namely, transparency,\n\ninterpretability, and explainability (Roscher et al. 2020). Transparency is concerned\n\nwith the ML approach, while interpretability is concerned with the ML model in\n\ncombination with the data, which means the making sense of the obtained ML\n\nmodel (Roscher et al. 2020). Finally, explainability comprises the model, the data,\n\nand human involvement (Roscher et al. 2020). Concerning the former, transparency\n\ncan be distinguished at three different levels: ‘‘[…] at the level of the entire model\n\n(simulatability), at the level of individual components, such as parameters\n\n(decomposability), and at the level of the training (algorithmic transparency)’’\n\n(Roscher et al. 2020, p. 4). Interpretability concerns the characteristics of an ML\n\nmodel that need to be understood by a human (Roscher et al. 2020). Finally, the\n\nelement of explainability is paramount in HRM. Contextual information of human\n\nand their knowledge from the domain of HRM are necessary to explain the different\n\nsets of interpretations and derive conclusions about the results of the algorithms\n\nBusiness Research (2020) 13:795–848 799\n\n123\n\n\n\n(Roscher et al. 2020). Especially in HRM, in which ML algorithms are increasingly\n\nused for prediction of variables of interest to the HR department (e.g., personality\n\ncharacteristics, employee satisfaction, and turnover intentions), it is essential to\n\nunderstand how the ML algorithm operates (e.g., how the ML algorithm uses data\n\nand weighs specific criteria) and the underlying reasons for the produced decision.\n\nIn the following, we will outline the main reasons for biases in algorithmic\n\ndecision-making and briefly summarize different biases, namely historical, repre-\n\nsentation, technical, and emergent bias. One of the main reasons for bias in\n\nalgorithmic decision-making is the quality of input data, because algorithms learn\n\nfrom historical data as an example; thus, the learning process depends on the\n\nexposed examples (Friedman and Nissenbaum 1996; Barocas and Selbst 2016;\n\nDanks and London 2017). The input data are usually historical. Consequently, if the\n\ninput data set is biased in one way or another, the subsequent analysis is biased, as\n\nwell (keyword: ‘‘garbage in, garbage out’’). For example, if the input data of an\n\nalgorithm include implicit or explicit human judgments, stereotypes, or biases, an\n\naccurate algorithmic output will inevitably entail these human judgments, stereo-\n\ntypes, and prejudices (Diakopoulos 2015; Suresh and Guttag 2019; Barfield and\n\nPagallo 2018). This bias usually exists before the creation of the system and may not\n\nbe apparent at first glance. In turn, the algorithm replicates these preexisting biases,\n\nbecause it treats all information, in which a certain kind of discrimination or bias is\n\nembedded, as a valid example (Barocas and Selbst 2016; Lindebaum et al. 2019). In\n\nthe worst case, the algorithm can yield racist or discriminatory outputs (Veale and\n\nBinns 2017). Algorithms exhibit these tendencies, even if it is not the intention of\n\nthe manual programming since they compound the historical biases of the past.\n\nThus, any predictive algorithmic decision-making tool built on historical data may\n\ninherit historical biases (Datta et al. 2015).\n\nAs an example from the recruitment process, if an algorithm is trained on\n\nhistorical employment data, integrating an implicit bias that favors white men over\n\nHispanics, then, without even being fed data on gender or ethnicity, an algorithm\n\nmay recognize patterns in the data, which expose an applicant as a member of a\n\ncertain protected group, which, historically, is less likely to be chosen for a job\n\ninterview. This, in turn, may lead to a systematic disadvantage of certain groups,\n\neven if the designer has no intention of marginalizing people based on these\n\ncategories and if the algorithm is not directly given this information (Barocas and\n\nSelbst 2016).\n\nAnother reason for biases in algorithms related to the input data is that certain\n\ngroups or characteristics are mostly underrepresented or sometimes overrepre-\n\nsented, which is also called representation bias (Barocas and Selbst 2016; Suresh\n\nand Guttag 2019; Barfield and Pagallo 2018). Any decision based on this kind of\n\nbiased data might lead to disadvantages of groups of individuals who are\n\nunderrepresented or overrepresented (Barocas and Selbst 2016). Another reason\n\nfor representation bias can be the absence of specific information (Barfield and\n\nPagallo 2018). Thus, not only the selection of measurements but also the\n\npreprocessing of the measurement data might yield to bias. ML models often\n\nevolve in several steps of feature engineering or model testing, since there is no\n\nuniversally best model (as shown in the ‘‘no free lunch’’ theorems, [see Wolpert and\n\n800 Business Research (2020) 13:795–848\n\n123\n\n\n\nMacready (1997)]. Here, the choice of the benchmark or rather the value indicating\n\nthe performance of the model is optimized through rotations of different\n\nrepresentations of the data and methods for prediction. For example, representative\n\nbias might occur if females in comparison to males are underrepresented in the\n\ntraining data of an algorithm. Hence, the outcome could be in favor of the\n\noverrepresented group (i.e., males) and, hence, lead to discriminatory outcomes.\n\nTechnical bias may arise from technical constraints or technical consideration for\n\nseveral reasons. For example, technical bias can originate from limited ‘‘[…]\n\ncomputer technology, including hardware, software, and peripherals’’ (Friedman\n\nand Nissenbaum 1996, p. 334). Another reason could be a decontextualized\n\nalgorithm that does not manage to treat all groups fairly under all important\n\nconditions (Friedman and Nissenbaum 1996; Bozdag 2013). The formalization of\n\nhuman constructs to computers can be another problem leading to technical bias.\n\nHuman constructs, such as judgments or intuitions, are often hard to quantify, which\n\nmakes it difficult or even impossible to translate them to the computer (Friedman\n\nand Nissenbaum 1996). As an example, the human interpretation of law can be\n\nambiguous and highly dependent on the specific context, making it difficult for an\n\nalgorithmic system to correctly advise in litigation (c.f., Friedman and Nissenbaum\n\n1996).\n\nIn the context of real users, emergent bias may arise. Typically, this bias occurs\n\nafter the construction as a result of changed societal knowledge, population, or\n\ncultural values (Friedman and Nissenbaum 1996). Consequently, a shift in the\n\ncontext of use might yield to problems and an emergent bias due to two reasons,\n\nnamely ‘‘new societal knowledge’’ and ‘‘mismatch between users and system\n\ndesign’’ (see Table 1 in Friedman and Nissenbaum 1996, p. 335). If it is not possible\n\nto incorporate new knowledge in society into the system design, emergent bias due\n\nto new societal knowledge occurs. The mismatch between users and system design\n\ncan occur due to changes in state-of-the-art-research or due to different values. Also,\n\nemergent bias can occur if a population uses the system with different values than\n\nthose assumed in the design process (Friedman and Nissenbaum 1996). Problems\n\noccur, for example, when users originate from a cultural context that avoids\n\ncompetition and promotes cooperative efforts, while the algorithm is trained to\n\nreward individualistic and competitive behavior (Friedman and Nissenbaum 1996).\n\n2.3 Fairness and discrimination in information systems\n\nLeventhal (1980) describes fairness as equal treatment based on people’s\n\nperformance and needs. Table 1 offers an overview of the different fairness\n\ndefinitions. Individual fairness means that, independent of group membership, two\n\nindividuals who are perceived to be similar by the measures at hand should also be\n\ntreated similarly (Dwork et al. 2012). Rising from the micro-level onto the meso-\n\nlevel, Dwork et al. (2012) also proposed another measure of fairness, that is, group\n\nfairness, in which entire (protected) groups of people are required to be treated\n\nsimilarly (statistical parity). Hardt et al. (2016) extended these notions by including\n\ntrue outcomes of predicted variables to achieve fair treatment. In their sense, false-\n\nBusiness Research (2020) 13:795–848 801\n\n123\n\n\n\npositives/negatives are sources of disadvantage and should be equal among groups\n\nmeans equal opportunity for false-positives/negatives (Hardt et al. 2016).\n\nUnfair treatment of certain groups of people or individual subjects yields to\n\ndiscrimination. Discrimination is defined as the unequal treatment of different\n\ngroups (Arrow 1973). Discrimination is very similar to unfairness. Discriminatory\n\ncategories can be strongly correlated with non-discriminatory categories, such as\n\nage (i.e., discriminatory) and years of working experience (non-discriminatory)\n\n(Persson 2016). Also, there is a difference between implicit and explicit\n\ndiscrimination. Implicit discrimination is based on implicit attitudes or stereotypes\n\nand often unintentional (Bertrand et al. 2005). In contrast, explicit discrimination is\n\na conscious process due to an aversion to certain groups of people. In HR\n\nrecruitment and HR development, discrimination means the not-hiring or support of\n\na person due to characteristics not related to that person’s productivity in the current\n\nposition (Frijters 1998).\n\nThe HR literature, especially the literature on personnel selection, is concerned\n\nwith fairness in hiring decisions, because every selection measure of individual\n\ndifferences is inevitably discriminatory (Cascio and Aguinis 2013). However, the\n\nquestion arises ‘‘whether the measure discriminates unfairly’’ (Cascio and Aguinis\n\n2013, p. 183). Hence, the actual fairness of prediction systems needs to be tested\n\nbased on probabilities and estimates, which we refer to as objective fairness. In the\n\nselection context, the literature distinguishes between differential validity (i.e.,\n\ndifferences in subgroup validity) and differential prediction (i.e., differences in\n\nslopes and intercepts of subgroups), and both might lead to biased results (Meade\n\nand Fetzer 2009; Roth et al. 2017; Bobko and Bartlett 1978).\n\nIn HR recruitment and HR development, both objective fairness and subjective\n\nfairness perceptions of applicants and employees about the usage of algorithmic\n\ndecision-making need to be considered. In this regard, perceived fairness or justice\n\nis more a subjective and descriptive personal evaluation rather than an objective\n\nreality (Cropanzano et al. 2007). Subjective fairness plays an essential role in the\n\nrelationship between humans and their employers. Previous studies showed that the\n\nTable 1 Definitions of fairness\n\nName Author Definition\n\nIndividual\n\nfairness\n\nDwork et al.\n\n(2012)\n\n‘‘Similar’’ subjects should have ‘‘similar’’ classifications\n\nGroup\n\nfairness\n\nSubjects in protected and unprotected groups have an equal probability\n\nof being assigned positive\n\nP bY ¼ 1\n� �\n\n�\n\n�G ¼ 1Þ ¼ Pð bY ¼ 1jG ¼ 0Þ\n\nEqual\n\nopportunity\n\nHardt et al.\n\n(2016)\n\nFalse-negative rates should be equal\n\nP bY ¼ 0\n� �\n\n�\n\n�Y ¼ 1;G ¼ 1Þ ¼ Pð bY ¼ 0jY ¼ 1;G ¼ 0Þ\n\nY 2 0; 1f g is a random variable describing, e.g., the recidivism of a subject, bY its estimator and G 2\nf0; 1g; describes whether a subject is a member of a certain protected group (G ¼ 1Þ or not ðG ¼ 0Þ\n\n802 Business Research (2020) 13:795–848\n\n123\n\n\n\nlikelihood of conscientious behavior and altruisms is higher for employees who feel\n\ntreated fairly (Cohen-Charash and Spector 2001). Conversely, unfairness can have\n\nconsiderable adverse consequences. For example, in the recruitment context,\n\nfairness perceptions of candidates during the selection process have important\n\nconsequences for decision to stay in the applicant pool or accept a job offer (Bauer\n\net al. 2001). Therefore, it is crucial to know how people feel about algorithmic\n\ndecision-making taking over managerial decisions formerly made by humans, since\n\nthe fairness perceptions during the recruitment process and/or training process have\n\nessential and meaningful effects on attitudes, performance, morale, intentions, and\n\nbehavior (e.g., the acceptance or rejection of a job offer or job turnover, job\n\ndissatisfaction, and reduction or elimination of conflicts) (Gilliland 1993; McCarthy\n\net al. 2017; Hausknecht et al. 2004; Cropanzano et al. 2007; Cohen-Charash and\n\nSpector 2001). Moreover, negative experiences might damage the employer�s\nimage. Several online platforms offer the possibility of rating companies and their\n\nrecruitment and development process (Van Hoye 2013; Woods et al. 2020).\n\nConsidering justice and fairness in the organizational context (Gilliland 1993),\n\nthere are three core dimensions of justice: distributive, procedural, and interactional.\n\nThe three dimensions tend to be correlated. Distributive justice deals with the\n\noutcome that some humans receive and some do not (Cropanzano et al. 2007). Rules\n\nthat can lead to distributive justice are ‘‘[…] equality (to each the same), equity (to\n\neach in accordance with contributions, and need (to each in accordance with the\n\nmost urgency)’’ (Cropanzano et al. 2007, p. 37). To some extent, especially\n\nconcerning equity, this can be connected with individual fairness and group fairness\n\nfrom Dwork et al. (2012) and equal opportunities from Hardt et al. (2016).\n\nProcedural justice means that the process is consistent with all humans, not\n\nincluding bias, accurate, and consistent with the ethical norms (Cropanzano et al.\n\n2007; Leventhal 1980). Consistency plays an essential role in procedural justice,\n\nmeaning that all employees and all candidates need to receive the same treatment.\n\nAdditionally, the lack of bias, accuracy, representation of all parties, correction, and\n\nethics play an important role in achieving a high procedural justice (Cropanzano\n\net al. 2007). In contrast, interactional justice is about the treatment of humans,\n\nmeaning the appropriateness of the treatment from another member of the company,\n\nthe treatment with dignity, courtesy, and respect, and informational justice (share of\n\nrelevant information) (Cropanzano et al. 2007).\n\nIn general, algorithmic decision-making increases the standardization of\n\nprocedures, so that decisions should be more objective and less biased, and errors\n\nshould occur less frequently (Kaibel et al. 2019), since information processing by\n\nhuman raters can be unsystematic, leading to contradictory and insufficient\n\nevidence-based decisions (Woods et al. 2020). Consequently, procedural justice and\n\ndistributive justice are higher using algorithmic decision-making, because the\n\nprocess is more standardized, which still not means that it is without bias.\n\nHowever, especially in the context of an application or an employee evaluation, it\n\nis not only about how fair the procedure itself is (according to fairness measures),\n\nbut it is also about how people involved in the decision process perceive the fairness\n\nof the whole process. Often the personal contact, which characterizes the\n\nBusiness Research (2020) 13:795–848 803\n\n123\n\n\n\ninteractional fairness, is missing when using algorithmic decision-making. It is\n\ndifficult to fulfill all three fairness dimensions.\n\n3 Methods\n\nThis systematic literature review aims at offering a coherent, transparent, and\n\nreliable picture of existing knowledge and providing insights into fruitful research\n\navenues about the discrimination potential and fairness when using algorithmic\n\ndecision-making in HR recruitment and HR development. This is in line with other\n\nsystematic literature reviews that organize, evaluate, and synthesize knowledge in a\n\nparticular field and provide an overall picture of knowledge and suggestions for\n\nfuture research (Petticrew and Roberts 2008; Crossan and Apaydin 2010; Siddaway\n\net al. 2019). To this end, we followed the systematic literature review approach\n\ndescribed by Siddaway et al. (2019) and Gough et al. (2017) to ensure a methodical,\n\ntransparent, and replicable approach.1\n\n3.1 Search terms and databases\n\nWe engaged in an extensive keyword searching, which we derived in an iterative\n\nprocess of search and discussion between the two authors of this study (see\n\n‘‘Appendix’’ for the employed keywords). According to our research question, we\n\nfirst defined individual concepts to create search terms. We considered different\n\nterminology, including synonyms, singular/plural forms, different spellings, broader\n\nvs. narrow terms, and classification terms of databases to categorize contents\n\n(Siddaway et al. 2019) (see Table 2 for a complete list of employed keywords and\n\nsearch strings). Our priority was to achieve the balance between sensitivity and\n\nspecificity to get broad coverage of the literature and to avoid the unintentional\n\nomission of relevant articles (Siddaway et al. 2019).\n\nAs the first source of data, we used the social science citation index (SSCI) to\n\nensure broad coverage of scholarly literature. This database covers English-\n\nlanguage peer-reviewed journals in business and management. As part of the Web\n\nof Knowledge, the database includes all journals with an impact factor, which is a\n\nreasonable proxy for the most important publications in the field. We completed our\n\nsearch with the EBSCO Business Source Premier database to add further breadth.\n\nSince electronic databases are not fully comprehensive, we additionally searched in\n\nthe reference section of the considered papers and manually searched for articles\n\n(Siddaway et ",
      "metadata_storage_size": 593265,
      "metadata_storage_path": "aHR0cHM6Ly9qdHJhaW5pbmdjYXRhbG9nc3RvcmFnZS5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL0slQzMlQjZjaGxpbmctV2VobmVyMjAyMF9BcnRpY2xlX0Rpc2NyaW1pbmF0ZWRCeUFuQWxnb3JpdGhtQVN5cy5wZGY1",
      "metadata_author": "Alina Köchling ",
      "metadata_title": "Discriminated by an algorithm: a systematic review of discrimination and fairness by algorithmic decision-making in the context of HR recruitment and HR development",
      "metadata_creation_date": "2020-11-19T15:45:16Z",
      "people": [
        "Alina Köchling1",
        "Marius Claus Wehner1",
        "Alina Köchling",
        "Zalmanson",
        "Chalfin",
        "Lindebaum",
        "Silverman",
        "Waller",
        "Carey",
        "Smith",
        "Savage",
        "Bales",
        "Walker",
        "Wilson",
        "Suen",
        "McDonald",
        "McColl",
        "Michelotti",
        "Woods",
        "Raghavan",
        "Lepri",
        "Lee",
        "Simbeck",
        "Barocas",
        "Selbst",
        "Suresh",
        "Guttag",
        "Chander",
        "Dwork",
        "Miller",
        "Pasquale",
        "Kaibel",
        "Langer",
        "Huselid",
        "Ötting",
        "Maier",
        "Tambe",
        "Cappelli",
        "Möhlmann",
        "Russell",
        "Norvig",
        "Paschen",
        "Shin",
        "Murphy",
        "Kael",
        "bling",
        "Bengio",
        "Deng",
        "Yu",
        "Kauermann",
        "Kuechenhoff",
        "Goodfellow",
        "Kahneman",
        "Friedman",
        "Nissenbaum",
        "Cheng",
        "Hackett",
        "Roscher",
        "Danks",
        "Diakopoulos",
        "Veale",
        "Datta",
        "Barfield",
        "Pagallo",
        "Wolpert",
        "Leventhal",
        "Hardt",
        "Persson",
        "Bertrand",
        "Frijters",
        "Cascio",
        "Aguinis",
        "Meade",
        "Fetzer",
        "Roth",
        "Bobko",
        "Bartlett",
        "Cropanzano",
        "Cohen-Charash",
        "Spector",
        "Gilliland",
        "McCarthy",
        "Hausknecht",
        "Petticrew",
        "Roberts",
        "Crossan",
        "Apaydin",
        "Siddaway",
        "Gough"
      ],
      "organizations": [
        "human resource management",
        "HRM",
        "Administration",
        "Economics",
        "Heinrich-Heine-University Düsseldorf",
        "Möhlmann",
        "Deloitte",
        "Google",
        "IBM",
        "SAP",
        "Microsoft",
        "Vodafone",
        "Intel",
        "Unilever",
        "Ikea",
        "Daugherty",
        "Arrow",
        "Kim",
        "Amazon",
        "Dastin",
        "Citron",
        "Pasquale",
        "company",
        "workplace",
        "Kaplan",
        "Haenlein",
        "department",
        "fairness",
        "Group",
        "opportunity",
        "Business Research",
        "Bauer",
        "procedural justice",
        "fairness measures",
        "EBSCO"
      ],
      "locations": [
        "algo",
        "German",
        "Florentine",
        "Deloitte",
        "Diakopoulos",
        "American",
        "neous state",
        "field",
        "Canhoto",
        "ML",
        "London",
        "Barfield",
        "Binns",
        "Hispanics",
        "Guttag",
        "Bozdag",
        "bY",
        "Van Hoye",
        "Leventhal",
        "Cropanzano",
        "Siddaway",
        "English"
      ],
      "keyphrases": [
        "200 artificial intelligence (AI) specialists",
        "two essential HR functions",
        "Marius Claus Wehner1",
        "Heinrich-Heine-University Düsseldorf",
        "major driving forces",
        "human resource management",
        "Alina Köchling1",
        "routinized workplace decisions",
        "current human resource",
        "crucial future research",
        "HRM � Literature review",
        "Abstract Algorithmic decision-making",
        "Keywords Fairness � Discrimination",
        "human biases",
        "systematic review",
        "HR recruitment",
        "HR development",
        "HR) practices",
        "ORIGINAL RESEARCH",
        "new source",
        "unfair treatment",
        "implicit discrimination",
        "implicit) discrimination",
        "current state",
        "research gaps",
        "36 journal articles",
        "possible pitfalls",
        "important theoretical",
        "practical implications",
        "fruitful avenues",
        "fairness � Ethics",
        "rapid growth",
        "Business Administration",
        "Universitätsstrasse",
        "Business Research",
        "automated decision-making",
        "remote control",
        "Möhlmann",
        "important individual",
        "societal implications",
        "organizational optimization",
        "large number",
        "German companies",
        "competitive advantages",
        "commercial providers",
        "algorithmic platforms",
        "performance measurements",
        "large companies",
        "economic reasons",
        "personal beliefs",
        "doi.org",
        "orcid.org",
        "context",
        "Author",
        "advice",
        "firms",
        "costs",
        "efficiency",
        "objectivity",
        "groups",
        "people",
        "unfairness",
        "knowledge",
        "threats",
        "goal",
        "directions",
        "applications",
        "researchers",
        "practitioners",
        "1 Introduction",
        "information",
        "importance",
        "digitalization",
        "organizations",
        "koechling",
        "hhu",
        "1 Faculty",
        "Economics",
        "40225 Dusseldorf",
        "Germany",
        "crossmark",
        "crossref",
        "standardization",
        "Zalmanson",
        "Algorithms",
        "humans",
        "Chalfin",
        "Lee",
        "Lindebaum",
        "changes",
        "favor",
        "talented",
        "employees",
        "Silverman",
        "Waller",
        "Carey",
        "Smith",
        "Savage",
        "Bales",
        "survey",
        "Deloitte",
        "Several",
        "Google",
        "IBM",
        "SAP",
        "Microsoft",
        "systems",
        "hiring",
        "Walker",
        "turn",
        "Vodafone",
        "Unilever",
        "Ikea",
        "Daugherty",
        "Wilson",
        "Precire",
        "savings",
        "time",
        "risks",
        "productivity",
        "certainty",
        "Suen",
        "McDonald",
        "McColl",
        "Michelotti",
        "Woods",
        "prejudices",
        "15",
        "American e-commerce specialist",
        "algorithmic decision- making",
        "new research avenues",
        "HR development processes",
        "unrepresentative input data",
        "algorithmic decision-making system",
        "complete algorithmic decision-making",
        "training) data",
        "algorithmic outcomes",
        "same attention",
        "same requirements",
        "first glance",
        "human decision-making",
        "unequal treatment",
        "different groups",
        "qualitative differences",
        "individual performance",
        "biased outcomes",
        "biased decisions",
        "current debate",
        "extreme disadvantage",
        "796 Business Research",
        "Previous studies",
        "common procedures",
        "labor shortages",
        "one hand",
        "other side",
        "computer science",
        "neous state",
        "distinct challenges",
        "future research",
        "practical point",
        "known companies",
        "negative consequences",
        "potential downsides",
        "potential dangers",
        "possible threat",
        "prominent example",
        "female applicants",
        "hiring decision",
        "employees’ acceptance",
        "existing knowledge",
        "current literature",
        "hiring algorithms",
        "potential biases",
        "HRM context",
        "consistency",
        "fairness",
        "Langer",
        "Florentine",
        "Raghavan",
        "application",
        "criteria",
        "Lepri",
        "discrimination",
        "Simbeck",
        "gender",
        "ethnicity",
        "Arrow",
        "Kim",
        "Barocas",
        "Selbst",
        "Suresh",
        "Guttag",
        "Chander",
        "issue",
        "transparency",
        "Dwork",
        "Diakopoulos",
        "Amazon",
        "Dastin",
        "Miller",
        "lack",
        "accountability",
        "factors",
        "Citron",
        "Pasquale",
        "question",
        "Kaibel",
        "discrepancy",
        "enthusiasm",
        "panacea",
        "inefficiencies",
        "field",
        "infancy",
        "digitization",
        "automation",
        "view",
        "aim",
        "study",
        "awareness",
        "The Oxford Living Dictionary",
        "two important HR areas",
        "several different research areas",
        "various algorithmic decision-making tools",
        "several AI decision tools",
        "other problem-solving operations",
        "two HR functions",
        "explicit human interference",
        "natural language processing",
        "several research gaps",
        "future research avenues",
        "systematic literature review",
        "HR development methods",
        "future research directions",
        "several ways",
        "workplace decision",
        "human intelligence",
        "algorithmic selection",
        "career development",
        "future performance",
        "person-organization fit",
        "serious consequences",
        "Ötting",
        "existing body",
        "ethical issues",
        "illustrative examples",
        "timely topic",
        "enormous importance",
        "reputational risk",
        "ment process",
        "key terms",
        "descriptive analysis",
        "subsequent discussion",
        "theoretical implications",
        "Conceptual background",
        "computational mechanism",
        "umbrella term",
        "wide array",
        "machine learning",
        "image recognition",
        "existing literature",
        "AI applications",
        "current employees",
        "HRM field",
        "lacking focus",
        "statistical models",
        "new data",
        "present paper",
        "distributive fairness",
        "understanding",
        "end",
        "potential",
        "Huselid",
        "Decisions",
        "algorithms",
        "individuals",
        "company",
        "society",
        "ethics",
        "procedural",
        "Maier",
        "Tambe",
        "Cappelli",
        "bias",
        "fact",
        "Companies",
        "legal",
        "applicants",
        "reason",
        "guidance",
        "detailed",
        "definitions",
        "methodology",
        "illustration",
        "processes",
        "sets",
        "rules",
        "calculations",
        "computer",
        "routinized",
        "basis",
        "prescriptions",
        "acquisition",
        "environment",
        "Russell",
        "Norvig",
        "ML",
        "speech",
        "NLP",
        "Kaplan",
        "Haenlein",
        "Paschen",
        "2.1",
        "many AI decision-making tools",
        "three major types",
        "artificial neural networks",
        "random variable Y",
        "three core elements",
        "three different levels",
        "neural network models",
        "deep learning models",
        "fixed input/output data",
        "reinforcement type learning",
        "Unsupervised ML algorithms",
        "reinforcement learning",
        "ML models",
        "network structures",
        "probabilistic models",
        "unsupervised settings",
        "learning tasks",
        "ML approach",
        "regression-type problems",
        "ground truth",
        "Human experts",
        "human decisions",
        "future instances",
        "same problem",
        "priori labeling",
        "structural behaviors",
        "theme analysis",
        "798 Business Research",
        "separate group",
        "error interactions",
        "dynamic environment",
        "Single nodes",
        "human brain",
        "human thinking",
        "other settings",
        "prediction tasks",
        "expected values",
        "systematic error",
        "human judgment",
        "algorithmic evaluations",
        "computer systems",
        "black box",
        "glass boxes",
        "making sense",
        "human involvement",
        "Cognitive biases",
        "entire model",
        "nonlinear transformation",
        "individual components",
        "input data",
        "Shin",
        "predictions",
        "outputs",
        "labels",
        "patterns",
        "Canhoto",
        "contrast",
        "Murphy",
        "variables",
        "methods",
        "trial",
        "bling",
        "methodologies",
        "Examples",
        "complex",
        "relationship",
        "phases",
        "layers",
        "neurons",
        "Bengio",
        "multiple",
        "stages",
        "features",
        "representations",
        "advantage",
        "Deng",
        "Yu",
        "Reason",
        "estimation",
        "bY",
        "difference",
        "Kauermann",
        "Kuechenhoff",
        "Goodfellow",
        "uncertainty",
        "Kahneman",
        "others",
        "Friedman",
        "Nissenbaum",
        "HRM",
        "Cheng",
        "Hackett",
        "theory",
        "consideration",
        "distinction",
        "interpretability",
        "explainability",
        "Roscher",
        "combination",
        "former",
        "simulatability",
        "parameters",
        "2.2",
        "predictive algorithmic decision-making tool",
        "accurate algorithmic output",
        "explicit human judgments",
        "input data set",
        "historical employment data",
        "algorithmic transparency",
        "historical data",
        "ML model",
        "different sets",
        "HR department",
        "employee satisfaction",
        "turnover intentions",
        "specific criteria",
        "underlying reasons",
        "main reasons",
        "learning process",
        "exposed examples",
        "one way",
        "subsequent analysis",
        "worst case",
        "discriminatory outputs",
        "manual programming",
        "recruitment process",
        "white men",
        "protected group",
        "systematic disadvantage",
        "biased data",
        "historical biases",
        "different biases",
        "preexisting biases",
        "Contextual information",
        "emergent bias",
        "representation bias",
        "specific information",
        "personality characteristics",
        "ML algorithm",
        "valid example",
        "implicit bias",
        "decomposability",
        "level",
        "training",
        "Interpretability",
        "element",
        "domain",
        "interpretations",
        "derive",
        "conclusions",
        "results",
        "prediction",
        "interest",
        "technical",
        "quality",
        "Danks",
        "London",
        "keyword",
        "garbage",
        "stereotypes",
        "Barfield",
        "Pagallo",
        "creation",
        "kind",
        "racist",
        "Veale",
        "Binns",
        "tendencies",
        "past",
        "Datta",
        "Hispanics",
        "applicant",
        "member",
        "job",
        "interview",
        "designer",
        "categories",
        "disadvantages",
        "absence",
        "selection",
        "measurements",
        "information systems Leventhal",
        "new societal knowledge",
        "different fairness definitions",
        "new knowledge",
        "several steps",
        "feature engineering",
        "free lunch",
        "discriminatory outcomes",
        "technical constraints",
        "technical consideration",
        "several reasons",
        "important conditions",
        "human constructs",
        "human interpretation",
        "cultural values",
        "two reasons",
        "different values",
        "design process",
        "cooperative efforts",
        "competitive behavior",
        "equal treatment",
        "meso- level",
        "statistical parity",
        "true outcomes",
        "fair treatment",
        "equal opportunity",
        "individual subjects",
        "representative bias",
        "Technical bias",
        "algorithmic system",
        "system design",
        "measurement data",
        "model testing",
        "best model",
        "800 Business Research",
        "training data",
        "Individual fairness",
        "group membership",
        "computer technology",
        "specific context",
        "cultural context",
        "group fairness",
        "real users",
        "protected) groups",
        "preprocessing",
        "theorems",
        "Wolpert",
        "Macready",
        "choice",
        "benchmark",
        "performance",
        "rotations",
        "example",
        "females",
        "comparison",
        "overrepresented",
        "limited",
        "hardware",
        "software",
        "peripherals",
        "Bozdag",
        "formalization",
        "computers",
        "problem",
        "judgments",
        "intuitions",
        "law",
        "litigation",
        "construction",
        "result",
        "population",
        "shift",
        "mismatch",
        "Table",
        "state",
        "art",
        "competition",
        "individualistic",
        "needs",
        "overview",
        "measures",
        "hand",
        "micro-level",
        "Hardt",
        "notions",
        "sense",
        "positives/negatives",
        "sources",
        "disadvantage",
        "2.3",
        "fairness Name Author Definition Individual fairness Dwork",
        "similar’’ classifications Group fairness",
        "descriptive personal evaluation",
        "considerable adverse consequences",
        "individual differences",
        "actual fairness",
        "fairness perceptions",
        "objective fairness",
        "Subjective fairness",
        "Discriminatory categories",
        "working experience",
        "conscious process",
        "current position",
        "personnel selection",
        "prediction systems",
        "selection context",
        "differential validity",
        "subgroup validity",
        "differential prediction",
        "biased results",
        "Table 1 Definitions",
        "P bY",
        "Pð bY",
        "opportunity Hardt",
        "False-negative rates",
        "1f g",
        "random variable",
        "802 Business Research",
        "selection process",
        "applicant pool",
        "training process",
        "meaningful effects",
        "negative experiences",
        "recruitment context",
        "job offer",
        "job turnover",
        "explicit discrimination",
        "selection measure",
        "essential role",
        "equal probability",
        "conscientious behavior",
        "managerial decisions",
        "Implicit discrimination",
        "implicit attitudes",
        "HR literature",
        "years",
        "Persson",
        "Bertrand",
        "aversion",
        "support",
        "characteristics",
        "Frijters",
        "Cascio",
        "Aguinis",
        "probabilities",
        "estimates",
        "slopes",
        "intercepts",
        "subgroups",
        "Meade",
        "Fetzer",
        "Roth",
        "Bobko",
        "Bartlett",
        "usage",
        "algorithmic",
        "decision-making",
        "regard",
        "justice",
        "reality",
        "Cropanzano",
        "employers",
        "subjects",
        "positive",
        "1jG",
        "recidivism",
        "estimator",
        "likelihood",
        "altruisms",
        "Cohen-Charash",
        "Spector",
        "candidates",
        "Bauer",
        "morale",
        "intentions",
        "acceptance",
        "rejection",
        "dissatisfaction",
        "reduction",
        "elimination",
        "conflicts",
        "Gilliland",
        "McCarthy",
        "Hausknecht",
        "image",
        "systematic literature review approach",
        "systematic literature reviews",
        "Several online platforms",
        "extensive keyword searching",
        "defined individual concepts",
        "three core dimensions",
        "fruitful research avenues",
        "high procedural justice",
        "three fairness dimensions",
        "three dimensions",
        "replicable approach.1",
        "individual fairness",
        "research question",
        "rating companies",
        "Van Hoye",
        "most urgency",
        "equal opportunities",
        "ethical norms",
        "important role",
        "relevant information",
        "algorithmic decision-making",
        "information processing",
        "human raters",
        "employee evaluation",
        "personal contact",
        "reliable picture",
        "discrimination potential",
        "particular field",
        "overall picture",
        "two authors",
        "Distributive justice",
        "informational justice",
        "fairness measures",
        "Search terms",
        "interactional justice",
        "development process",
        "organizational context",
        "evidence-based decisions",
        "decision process",
        "iterative process",
        "interactional fairness",
        "same treatment",
        "possibility",
        "outcome",
        "Rules",
        "equality",
        "equity",
        "accordance",
        "contributions",
        "extent",
        "Leventhal",
        "Consistency",
        "accuracy",
        "representation",
        "parties",
        "correction",
        "appropriateness",
        "dignity",
        "courtesy",
        "respect",
        "share",
        "general",
        "procedures",
        "errors",
        "3 Methods",
        "insights",
        "other",
        "suggestions",
        "Petticrew",
        "Roberts",
        "Crossan",
        "Apaydin",
        "Siddaway",
        "Gough",
        "databases",
        "discussion",
        "keywords",
        "3.1",
        "EBSCO Business Source Premier database",
        "social science citation index",
        "language peer-reviewed journals",
        "first source",
        "singular/plural forms",
        "different spellings",
        "narrow terms",
        "classification terms",
        "complete list",
        "broad coverage",
        "impact factor",
        "reasonable proxy",
        "important publications",
        "reference section",
        "search strings",
        "relevant articles",
        "scholarly literature",
        "electronic databases",
        "terminology",
        "synonyms",
        "contents",
        "priority",
        "balance",
        "sensitivity",
        "specificity",
        "unintentional",
        "omission",
        "SSCI",
        "management",
        "part",
        "Web",
        "Knowledge",
        "breadth",
        "papers"
      ],
      "merged_content": "\nORIGINAL RESEARCH\n\nDiscriminated by an algorithm: a systematic review\nof discrimination and fairness by algorithmic decision-\nmaking in the context of HR recruitment and HR\ndevelopment\n\nAlina Köchling1\n• Marius Claus Wehner1\n\nReceived: 15 October 2019 / Accepted: 1 November 2020 / Published online: 20 November 2020\n\n� The Author(s) 2020\n\nAbstract Algorithmic decision-making is becoming increasingly common as a new\n\nsource of advice in HR recruitment and HR development. While firms implement\n\nalgorithmic decision-making to save costs as well as increase efficiency and\n\nobjectivity, algorithmic decision-making might also lead to the unfair treatment of\n\ncertain groups of people, implicit discrimination, and perceived unfairness. Current\n\nknowledge about the threats of unfairness and (implicit) discrimination by algo-\n\nrithmic decision-making is mostly unexplored in the human resource management\n\ncontext. Our goal is to clarify the current state of research related to HR recruitment\n\nand HR development, identify research gaps, and provide crucial future research\n\ndirections. Based on a systematic review of 36 journal articles from 2014 to 2020,\n\nwe present some applications of algorithmic decision-making and evaluate the\n\npossible pitfalls in these two essential HR functions. In doing this, we inform\n\nresearchers and practitioners, offer important theoretical and practical implications,\n\nand suggest fruitful avenues for future research.\n\nKeywords Fairness � Discrimination � Perceived fairness � Ethics �\nAlgorithmic decision-making in HRM � Literature review\n\n1 Introduction\n\nAlgorithmic decision-making in human resource management (HRM) is becoming\n\nincreasingly common as a new source of information and advice, and it will gain\n\nmore importance due to the rapid growth of digitalization in organizations.\n\n& Alina Köchling\n\nalina.koechling@hhu.de\n\n1 Faculty of Business Administration and Economics, Heinrich-Heine-University Düsseldorf,\n\nUniversitätsstrasse 1, 40225 Dusseldorf, Germany\n\n123\n\nBusiness Research (2020) 13:795–848\n\nhttps://doi.org/10.1007/s40685-020-00134-w\n\nhttp://orcid.org/0000-0001-7039-9852\nhttp://orcid.org/0000-0002-1932-3155\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40685-020-00134-w&amp;domain=pdf\nhttps://doi.org/10.1007/s40685-020-00134-w\n\n\nAlgorithmic decision-making is defined as automated decision-making and remote\n\ncontrol, as well as standardization of routinized workplace decisions (Möhlmann\n\nand Zalmanson 2017). Algorithms, instead of humans, make decisions, and this has\n\nimportant individual and societal implications in organizational optimization\n\n(Chalfin et al. 2016; Lee 2018; Lindebaum et al. 2019). These changes in favor\n\nof algorithmic decision-making make it easier to discover hidden talented\n\nemployees in organizations and review a large number of applications automatically\n\n(Silverman and Waller 2015; Carey and Smith 2016; Savage and Bales 2017). In a\n\nsurvey of 200 artificial intelligence (AI) specialists from German companies, 79%\n\nstated that AI is irreplaceable for competitive advantages (Deloitte 2020). Several\n\ncommercial providers, such as Google, IBM, SAP, and Microsoft, already offer\n\nalgorithmic platforms and systems that facilitate current human resource (HR)\n\npractices, such as hiring and performance measurements (Walker 2012). In turn,\n\nwell-known and large companies, such as Vodafone, Intel, Unilever, and Ikea, apply\n\nalgorithmic decision-making in HR recruitment and HR development (Daugherty\n\nand Wilson 2018; Precire 2020).\n\nThe major driving forces for algorithmic decision-making are savings in both\n\ncosts and time, minimizing risks, enhancing productivity, and increasing certainty in\n\ndecision-making (Suen et al. 2019; McDonald et al. 2017; McColl and Michelotti\n\n2019; Woods et al. 2020). Besides these economic reasons, firms seek to diminish\n\nthe human biases (e.g., prejudices and personal beliefs) by applying algorithmic\n\ndecision-making, thereby increasing the objectivity, consistency, and fairness of the\n\nHR recruitment as well as HR development processes (Langer et al. 2019;\n\nFlorentine 2016; Raghavan et al. 2020). For example, Deloitte argues that the\n\nalgorithmic decision-making system always manages each application with the\n\nsame attention according to the same requirements and criteria (Deloitte 2018). At\n\nfirst glance, algorithmic decision-making seems to be more objective and fairer than\n\nhuman decision-making (Lepri et al. 2018).\n\nHowever, there is a possible threat of discrimination and unfairness by relying\n\nsolely on algorithmic decision-making (e.g., (Lee 2018; Lindebaum et al. 2019;\n\nSimbeck 2019)). In general, discrimination is defined as the unequal treatment of\n\ndifferent groups based on gender, age, or ethnicity instead of on qualitative\n\ndifferences, such as individual performance (Arrow 1973). Algorithms produce\n\ndiscrimination or biased outcomes if they are trained on inaccurate (Kim 2016),\n\nbiased (Barocas and Selbst 2016), or unrepresentative input data (Suresh and Guttag\n\n2019). Consequently, algorithms are vulnerable to produce or replicate biased\n\ndecisions if their input (or training) data are biased (Chander 2016).\n\nComplicating this issue, biases and discrimination are often only recognized after\n\nalgorithms have made a decision. As a prominent example stemming from the\n\ncurrent debate around transparency, bias, and fairness in algorithmic decision-\n\nmaking (Dwork et al. 2012; Lepri et al. 2018; Diakopoulos 2015), the hiring\n\nalgorithms applied by the American e-commerce specialist Amazon yielded an\n\nextreme disadvantage of female applicants, which finally led Amazon to shut down\n\nthe complete algorithmic decision-making for their hiring decision (Dastin 2018;\n\nMiller 2015). Thus, the lack of transparency and accountability of the input data, the\n\nalgorithm itself, and the factors influencing algorithmic outcomes are potential\n\n796 Business Research (2020) 13:795–848\n\n123\n\n\n\nissues associated with algorithmic decision-making (Citron and Pasquale 2014;\n\nPasquale 2015). Another question remains whether applicants and/or employees\n\nperceive the algorithmic decision-making to be fair. Previous studies showed that\n\napplicants’ and employees’ acceptance of algorithmic decision-making is lower in\n\nHR recruitment and HR development compared to common procedures conducted\n\nby humans (Kaibel et al. 2019; Langer et al. 2019; Lee 2018).\n\nConsequently, there is a discrepancy between the enthusiasm about algorithmic\n\ndecision-making as a panacea for inefficiencies and labor shortages on one hand and\n\nthe threat of discrimination and unfairness of algorithmic decision-making on the\n\nother side. While the literature in the field of computer science has already\n\naddressed the issues of biases, knowledge about the potential downsides of\n\nalgorithmic decision-making is still in its infancy in the field of HRM despite its\n\nimportance due to increased digitization and automation in HRM. This heteroge-\n\nneous state of research on discrimination and fairness raises distinct challenges for\n\nfuture research. From a practical point of view, it is problematic if large and well-\n\nknown companies implement algorithms without being aware of the possible pitfalls\n\nand negative consequences. Thus, to move the field forward, it is paramount to\n\nsystematically review and synthesize existing knowledge about biases and\n\ndiscrimination in algorithmic decision-making and to offer new research avenues.\n\nThe aim of this study is threefold. First, this review creates an awareness of\n\npotential biases and discrimination resulting from algorithmic decision-making in\n\nthe context of HR recruitment and HR development. Second, this study contributes\n\nto the current literature by informing both researchers and practitioners about the\n\npotential dangers of algorithmic decision-making in the HRM context. Finally, we\n\nguide future research directions with an understanding of existing knowledge and\n\ngaps in the literature. To this end, the present paper conducts a systematic review of\n\nthe current literature with a focus on HR recruitment and HR development. These\n\ntwo HR functions deal with the potential of future and current employees and the\n\n(automatic) prediction of person-organization fit, career development, and future\n\nperformance (Huselid 1995; Walker 2012). Decisions made by algorithms and AI in\n\nthese two important HR areas have serious consequences for individuals, the\n\ncompany, and society concerning ethics and both procedural and distributive\n\nfairness (Ötting and Maier 2018; Lee 2018; Tambe et al. 2019; Cappelli et al. 2020).\n\nOur study contributes to the existing body of research in several ways. First, the\n\nsystematic literature review contributes to the literature by highlighting the current\n\ndebate on ethical issues associated with algorithmic decision-making, including bias\n\nand discrimination (Barocas and Selbst 2016). Second, our research provides\n\nillustrative examples of various algorithmic decision-making tools used in HR\n\nrecruitment, HR development, and their potential for discrimination and perceived\n\nfairness. Moreover, our systematic review underlines the fact that it is a timely topic\n\ngaining enormous importance. Companies will face legal and reputational risk if\n\ntheir HR recruitment and HR development methods turn out to be discriminatory,\n\nand applicants and employees may consider the algorithmic selection or develop-\n\nment process to be unfair.\n\nFor this reason, companies need to know that the use of algorithmic decision-\n\nmaking can yield to discrimination, unfairness, and dissatisfaction in the context of\n\nBusiness Research (2020) 13:795–848 797\n\n123\n\n\n\nHRM. We offer an understanding of how discrimination might arise when\n\nimplementing algorithmic decision-making. We try to give guidance on how\n\ndiscrimination and perceived unfairness could be avoided and provide detailed\n\ndirections for future research in the existing literature, especially in the HRM field.\n\nMoreover, we identify several research gaps, mainly a lacking focus on perceived\n\nfairness.\n\nThe paper is organized as follows: first, we give an understanding of key terms\n\nand definitions. Afterward, we present the methodology of our systematic literature\n\nreview accompanied by a descriptive analysis of the reviewed literature. This is\n\nfollowed by an illustration of the current state of knowledge on algorithmic\n\ndecision-making and subsequent discussion. Finally, we offer practical as well as\n\ntheoretical implications and outline future research avenues.\n\n2 Conceptual background and definitions\n\n2.1 Definition of algorithms\n\nThe Oxford Living Dictionary defines algorithms as ‘‘processes or sets of rules to be\n\nfollowed in calculations or other problem-solving operations, especially by a\n\ncomputer.’’ Möhlmann and Zalmanson (2017) refer to algorithmic decision-making\n\nas automated decision-making and remote control, and standardization of routinized\n\nworkplace decision. Thus, in this paper, we use the term algorithmic decision-\n\nmaking to describe a computational mechanism that autonomously makes decisions\n\nbased on rules and statistical models without explicit human interference (Lee\n\n2018). Algorithms are the basis for several AI decision tools.\n\nAI is an umbrella term for a wide array of models, methods, and prescriptions\n\nused to simulate human intelligence, often when it comes to collecting, processing,\n\nand acting on data. AI applications can apply rules, learn over time through the\n\nacquisition of new data and information, and adapt to changes in the environment\n\n(Russell and Norvig 2016). AI includes several different research areas, such as\n\nmachine learning (ML), speech and image recognition, and natural language\n\nprocessing (NLP) (Kaplan and Haenlein 2019; Paschen et al. 2020).\n\nAs mentioned, the basis for many AI decision-making tools used in HR are ML\n\nalgorithms, which can be categorized into three major types: supervised, unsuper-\n\nvised, and reinforcement learning (Lee and Shin 2020). Supervised ML algorithms\n\naim to make predictions (often divided into classification- or regression-type\n\nproblems), given the input data and desired outputs considered as the ground truth.\n\nHuman experts often provide these labels and thus provide the algorithm with the\n\nground truth. To replicate human decisions or to make predictions, the algorithm\n\nlearns patterns from the labeled data and develops rules, which can be applied for\n\nfuture instances for the same problem (Canhoto and Clear 2020). In contrast, in\n\nunsupervised ML, only input data are given, and the model learns patterns from the\n\ndata without a priori labeling (Murphy 2012). Unsupervised ML algorithms capture\n\nthe structural behaviors of variables in the input data for theme analysis or grouping\n\n798 Business Research (2020) 13:795–848\n\n123\n\n\n\ndata (Canhoto and Clear 2020). Finally, reinforcement learning, as a separate group\n\nof methods, is not based on fixed input/output data. Instead, the ML algorithm learns\n\nbehavior through trial-and-error interactions with a dynamic environment (Kael-\n\nbling et al. 1996).\n\nFurthermore, instead of grouping ML models as supervised, unsupervised, or\n\nreinforcement type learning, the methodologies of algorithms may also be used to\n\ncategorize ML models. Examples are probabilistic models, which may be used in\n\nsupervised or unsupervised settings (Murphy 2012), or deep learning models (Lee\n\nand Shin 2020), which rely on artificial neural networks and perform complex\n\nlearning tasks. In supervised settings, neural network models often determine the\n\nrelationship between input and output using network structures containing the so-\n\ncalled hidden layers, meaning phases of transformation of the input data. Single\n\nnodes of these layers (neurons) were first modeled after neurons in the human brain,\n\nand they resemble human thinking (Bengio et al. 2017). In other settings, deep\n\nlearning may be used, for instance, to (1) process information through multiple\n\nstages of nonlinear transformation; or (2) determine features, representations of the\n\ndata providing an advantage for, e.g., prediction tasks (Deng and Yu 2014).\n\n2.2 Reason for biases\n\nFor any estimation bY of a random variable Y , bias refers to the difference between\n\nthe expected values of bY and Y and is also referred to as systematic error\n\n(Kauermann and Kuechenhoff 2010; Goodfellow et al. 2016). Cognitive biases,\n\nspecifically, are systematic errors in human judgment when dealing with uncertainty\n\n(Kahneman et al. 1982). These cognitive biases are thought to be transferred to\n\nalgorithmic evaluations or predictions, where bias may refer to ‘‘computer systems\n\nthat systematically and unfairly discriminate against certain individuals or groups in\n\nfavor of others’’ (Friedman and Nissenbaum 1996, p. 332).\n\nAlgorithms are often characterized as ‘‘black box’’. In the context of HRM,\n\nCheng and Hackett (2019) characterize algorithms as ‘‘glass boxes’’, since some,\n\nbut not all, components of the theory are reflective. In this context, the consideration\n\nand distinction of the three core elements are necessary, namely, transparency,\n\ninterpretability, and explainability (Roscher et al. 2020). Transparency is concerned\n\nwith the ML approach, while interpretability is concerned with the ML model in\n\ncombination with the data, which means the making sense of the obtained ML\n\nmodel (Roscher et al. 2020). Finally, explainability comprises the model, the data,\n\nand human involvement (Roscher et al. 2020). Concerning the former, transparency\n\ncan be distinguished at three different levels: ‘‘[…] at the level of the entire model\n\n(simulatability), at the level of individual components, such as parameters\n\n(decomposability), and at the level of the training (algorithmic transparency)’’\n\n(Roscher et al. 2020, p. 4). Interpretability concerns the characteristics of an ML\n\nmodel that need to be understood by a human (Roscher et al. 2020). Finally, the\n\nelement of explainability is paramount in HRM. Contextual information of human\n\nand their knowledge from the domain of HRM are necessary to explain the different\n\nsets of interpretations and derive conclusions about the results of the algorithms\n\nBusiness Research (2020) 13:795–848 799\n\n123\n\n\n\n(Roscher et al. 2020). Especially in HRM, in which ML algorithms are increasingly\n\nused for prediction of variables of interest to the HR department (e.g., personality\n\ncharacteristics, employee satisfaction, and turnover intentions), it is essential to\n\nunderstand how the ML algorithm operates (e.g., how the ML algorithm uses data\n\nand weighs specific criteria) and the underlying reasons for the produced decision.\n\nIn the following, we will outline the main reasons for biases in algorithmic\n\ndecision-making and briefly summarize different biases, namely historical, repre-\n\nsentation, technical, and emergent bias. One of the main reasons for bias in\n\nalgorithmic decision-making is the quality of input data, because algorithms learn\n\nfrom historical data as an example; thus, the learning process depends on the\n\nexposed examples (Friedman and Nissenbaum 1996; Barocas and Selbst 2016;\n\nDanks and London 2017). The input data are usually historical. Consequently, if the\n\ninput data set is biased in one way or another, the subsequent analysis is biased, as\n\nwell (keyword: ‘‘garbage in, garbage out’’). For example, if the input data of an\n\nalgorithm include implicit or explicit human judgments, stereotypes, or biases, an\n\naccurate algorithmic output will inevitably entail these human judgments, stereo-\n\ntypes, and prejudices (Diakopoulos 2015; Suresh and Guttag 2019; Barfield and\n\nPagallo 2018). This bias usually exists before the creation of the system and may not\n\nbe apparent at first glance. In turn, the algorithm replicates these preexisting biases,\n\nbecause it treats all information, in which a certain kind of discrimination or bias is\n\nembedded, as a valid example (Barocas and Selbst 2016; Lindebaum et al. 2019). In\n\nthe worst case, the algorithm can yield racist or discriminatory outputs (Veale and\n\nBinns 2017). Algorithms exhibit these tendencies, even if it is not the intention of\n\nthe manual programming since they compound the historical biases of the past.\n\nThus, any predictive algorithmic decision-making tool built on historical data may\n\ninherit historical biases (Datta et al. 2015).\n\nAs an example from the recruitment process, if an algorithm is trained on\n\nhistorical employment data, integrating an implicit bias that favors white men over\n\nHispanics, then, without even being fed data on gender or ethnicity, an algorithm\n\nmay recognize patterns in the data, which expose an applicant as a member of a\n\ncertain protected group, which, historically, is less likely to be chosen for a job\n\ninterview. This, in turn, may lead to a systematic disadvantage of certain groups,\n\neven if the designer has no intention of marginalizing people based on these\n\ncategories and if the algorithm is not directly given this information (Barocas and\n\nSelbst 2016).\n\nAnother reason for biases in algorithms related to the input data is that certain\n\ngroups or characteristics are mostly underrepresented or sometimes overrepre-\n\nsented, which is also called representation bias (Barocas and Selbst 2016; Suresh\n\nand Guttag 2019; Barfield and Pagallo 2018). Any decision based on this kind of\n\nbiased data might lead to disadvantages of groups of individuals who are\n\nunderrepresented or overrepresented (Barocas and Selbst 2016). Another reason\n\nfor representation bias can be the absence of specific information (Barfield and\n\nPagallo 2018). Thus, not only the selection of measurements but also the\n\npreprocessing of the measurement data might yield to bias. ML models often\n\nevolve in several steps of feature engineering or model testing, since there is no\n\nuniversally best model (as shown in the ‘‘no free lunch’’ theorems, [see Wolpert and\n\n800 Business Research (2020) 13:795–848\n\n123\n\n\n\nMacready (1997)]. Here, the choice of the benchmark or rather the value indicating\n\nthe performance of the model is optimized through rotations of different\n\nrepresentations of the data and methods for prediction. For example, representative\n\nbias might occur if females in comparison to males are underrepresented in the\n\ntraining data of an algorithm. Hence, the outcome could be in favor of the\n\noverrepresented group (i.e., males) and, hence, lead to discriminatory outcomes.\n\nTechnical bias may arise from technical constraints or technical consideration for\n\nseveral reasons. For example, technical bias can originate from limited ‘‘[…]\n\ncomputer technology, including hardware, software, and peripherals’’ (Friedman\n\nand Nissenbaum 1996, p. 334). Another reason could be a decontextualized\n\nalgorithm that does not manage to treat all groups fairly under all important\n\nconditions (Friedman and Nissenbaum 1996; Bozdag 2013). The formalization of\n\nhuman constructs to computers can be another problem leading to technical bias.\n\nHuman constructs, such as judgments or intuitions, are often hard to quantify, which\n\nmakes it difficult or even impossible to translate them to the computer (Friedman\n\nand Nissenbaum 1996). As an example, the human interpretation of law can be\n\nambiguous and highly dependent on the specific context, making it difficult for an\n\nalgorithmic system to correctly advise in litigation (c.f., Friedman and Nissenbaum\n\n1996).\n\nIn the context of real users, emergent bias may arise. Typically, this bias occurs\n\nafter the construction as a result of changed societal knowledge, population, or\n\ncultural values (Friedman and Nissenbaum 1996). Consequently, a shift in the\n\ncontext of use might yield to problems and an emergent bias due to two reasons,\n\nnamely ‘‘new societal knowledge’’ and ‘‘mismatch between users and system\n\ndesign’’ (see Table 1 in Friedman and Nissenbaum 1996, p. 335). If it is not possible\n\nto incorporate new knowledge in society into the system design, emergent bias due\n\nto new societal knowledge occurs. The mismatch between users and system design\n\ncan occur due to changes in state-of-the-art-research or due to different values. Also,\n\nemergent bias can occur if a population uses the system with different values than\n\nthose assumed in the design process (Friedman and Nissenbaum 1996). Problems\n\noccur, for example, when users originate from a cultural context that avoids\n\ncompetition and promotes cooperative efforts, while the algorithm is trained to\n\nreward individualistic and competitive behavior (Friedman and Nissenbaum 1996).\n\n2.3 Fairness and discrimination in information systems\n\nLeventhal (1980) describes fairness as equal treatment based on people’s\n\nperformance and needs. Table 1 offers an overview of the different fairness\n\ndefinitions. Individual fairness means that, independent of group membership, two\n\nindividuals who are perceived to be similar by the measures at hand should also be\n\ntreated similarly (Dwork et al. 2012). Rising from the micro-level onto the meso-\n\nlevel, Dwork et al. (2012) also proposed another measure of fairness, that is, group\n\nfairness, in which entire (protected) groups of people are required to be treated\n\nsimilarly (statistical parity). Hardt et al. (2016) extended these notions by including\n\ntrue outcomes of predicted variables to achieve fair treatment. In their sense, false-\n\nBusiness Research (2020) 13:795–848 801\n\n123\n\n\n\npositives/negatives are sources of disadvantage and should be equal among groups\n\nmeans equal opportunity for false-positives/negatives (Hardt et al. 2016).\n\nUnfair treatment of certain groups of people or individual subjects yields to\n\ndiscrimination. Discrimination is defined as the unequal treatment of different\n\ngroups (Arrow 1973). Discrimination is very similar to unfairness. Discriminatory\n\ncategories can be strongly correlated with non-discriminatory categories, such as\n\nage (i.e., discriminatory) and years of working experience (non-discriminatory)\n\n(Persson 2016). Also, there is a difference between implicit and explicit\n\ndiscrimination. Implicit discrimination is based on implicit attitudes or stereotypes\n\nand often unintentional (Bertrand et al. 2005). In contrast, explicit discrimination is\n\na conscious process due to an aversion to certain groups of people. In HR\n\nrecruitment and HR development, discrimination means the not-hiring or support of\n\na person due to characteristics not related to that person’s productivity in the current\n\nposition (Frijters 1998).\n\nThe HR literature, especially the literature on personnel selection, is concerned\n\nwith fairness in hiring decisions, because every selection measure of individual\n\ndifferences is inevitably discriminatory (Cascio and Aguinis 2013). However, the\n\nquestion arises ‘‘whether the measure discriminates unfairly’’ (Cascio and Aguinis\n\n2013, p. 183). Hence, the actual fairness of prediction systems needs to be tested\n\nbased on probabilities and estimates, which we refer to as objective fairness. In the\n\nselection context, the literature distinguishes between differential validity (i.e.,\n\ndifferences in subgroup validity) and differential prediction (i.e., differences in\n\nslopes and intercepts of subgroups), and both might lead to biased results (Meade\n\nand Fetzer 2009; Roth et al. 2017; Bobko and Bartlett 1978).\n\nIn HR recruitment and HR development, both objective fairness and subjective\n\nfairness perceptions of applicants and employees about the usage of algorithmic\n\ndecision-making need to be considered. In this regard, perceived fairness or justice\n\nis more a subjective and descriptive personal evaluation rather than an objective\n\nreality (Cropanzano et al. 2007). Subjective fairness plays an essential role in the\n\nrelationship between humans and their employers. Previous studies showed that the\n\nTable 1 Definitions of fairness\n\nName Author Definition\n\nIndividual\n\nfairness\n\nDwork et al.\n\n(2012)\n\n‘‘Similar’’ subjects should have ‘‘similar’’ classifications\n\nGroup\n\nfairness\n\nSubjects in protected and unprotected groups have an equal probability\n\nof being assigned positive\n\nP bY ¼ 1\n� �\n\n�\n\n�G ¼ 1Þ ¼ Pð bY ¼ 1jG ¼ 0Þ\n\nEqual\n\nopportunity\n\nHardt et al.\n\n(2016)\n\nFalse-negative rates should be equal\n\nP bY ¼ 0\n� �\n\n�\n\n�Y ¼ 1;G ¼ 1Þ ¼ Pð bY ¼ 0jY ¼ 1;G ¼ 0Þ\n\nY 2 0; 1f g is a random variable describing, e.g., the recidivism of a subject, bY its estimator and G 2\nf0; 1g; describes whether a subject is a member of a certain protected group (G ¼ 1Þ or not ðG ¼ 0Þ\n\n802 Business Research (2020) 13:795–848\n\n123\n\n\n\nlikelihood of conscientious behavior and altruisms is higher for employees who feel\n\ntreated fairly (Cohen-Charash and Spector 2001). Conversely, unfairness can have\n\nconsiderable adverse consequences. For example, in the recruitment context,\n\nfairness perceptions of candidates during the selection process have important\n\nconsequences for decision to stay in the applicant pool or accept a job offer (Bauer\n\net al. 2001). Therefore, it is crucial to know how people feel about algorithmic\n\ndecision-making taking over managerial decisions formerly made by humans, since\n\nthe fairness perceptions during the recruitment process and/or training process have\n\nessential and meaningful effects on attitudes, performance, morale, intentions, and\n\nbehavior (e.g., the acceptance or rejection of a job offer or job turnover, job\n\ndissatisfaction, and reduction or elimination of conflicts) (Gilliland 1993; McCarthy\n\net al. 2017; Hausknecht et al. 2004; Cropanzano et al. 2007; Cohen-Charash and\n\nSpector 2001). Moreover, negative experiences might damage the employer�s\nimage. Several online platforms offer the possibility of rating companies and their\n\nrecruitment and development process (Van Hoye 2013; Woods et al. 2020).\n\nConsidering justice and fairness in the organizational context (Gilliland 1993),\n\nthere are three core dimensions of justice: distributive, procedural, and interactional.\n\nThe three dimensions tend to be correlated. Distributive justice deals with the\n\noutcome that some humans receive and some do not (Cropanzano et al. 2007). Rules\n\nthat can lead to distributive justice are ‘‘[…] equality (to each the same), equity (to\n\neach in accordance with contributions, and need (to each in accordance with the\n\nmost urgency)’’ (Cropanzano et al. 2007, p. 37). To some extent, especially\n\nconcerning equity, this can be connected with individual fairness and group fairness\n\nfrom Dwork et al. (2012) and equal opportunities from Hardt et al. (2016).\n\nProcedural justice means that the process is consistent with all humans, not\n\nincluding bias, accurate, and consistent with the ethical norms (Cropanzano et al.\n\n2007; Leventhal 1980). Consistency plays an essential role in procedural justice,\n\nmeaning that all employees and all candidates need to receive the same treatment.\n\nAdditionally, the lack of bias, accuracy, representation of all parties, correction, and\n\nethics play an important role in achieving a high procedural justice (Cropanzano\n\net al. 2007). In contrast, interactional justice is about the treatment of humans,\n\nmeaning the appropriateness of the treatment from another member of the company,\n\nthe treatment with dignity, courtesy, and respect, and informational justice (share of\n\nrelevant information) (Cropanzano et al. 2007).\n\nIn general, algorithmic decision-making increases the standardization of\n\nprocedures, so that decisions should be more objective and less biased, and errors\n\nshould occur less frequently (Kaibel et al. 2019), since information processing by\n\nhuman raters can be unsystematic, leading to contradictory and insufficient\n\nevidence-based decisions (Woods et al. 2020). Consequently, procedural justice and\n\ndistributive justice are higher using algorithmic decision-making, because the\n\nprocess is more standardized, which still not means that it is without bias.\n\nHowever, especially in the context of an application or an employee evaluation, it\n\nis not only about how fair the procedure itself is (according to fairness measures),\n\nbut it is also about how people involved in the decision process perceive the fairness\n\nof the whole process. Often the personal contact, which characterizes the\n\nBusiness Research (2020) 13:795–848 803\n\n123\n\n\n\ninteractional fairness, is missing when using algorithmic decision-making. It is\n\ndifficult to fulfill all three fairness dimensions.\n\n3 Methods\n\nThis systematic literature review aims at offering a coherent, transparent, and\n\nreliable picture of existing knowledge and providing insights into fruitful research\n\navenues about the discrimination potential and fairness when using algorithmic\n\ndecision-making in HR recruitment and HR development. This is in line with other\n\nsystematic literature reviews that organize, evaluate, and synthesize knowledge in a\n\nparticular field and provide an overall picture of knowledge and suggestions for\n\nfuture research (Petticrew and Roberts 2008; Crossan and Apaydin 2010; Siddaway\n\net al. 2019). To this end, we followed the systematic literature review approach\n\ndescribed by Siddaway et al. (2019) and Gough et al. (2017) to ensure a methodical,\n\ntransparent, and replicable approach.1\n\n3.1 Search terms and databases\n\nWe engaged in an extensive keyword searching, which we derived in an iterative\n\nprocess of search and discussion between the two authors of this study (see\n\n‘‘Appendix’’ for the employed keywords). According to our research question, we\n\nfirst defined individual concepts to create search terms. We considered different\n\nterminology, including synonyms, singular/plural forms, different spellings, broader\n\nvs. narrow terms, and classification terms of databases to categorize contents\n\n(Siddaway et al. 2019) (see Table 2 for a complete list of employed keywords and\n\nsearch strings). Our priority was to achieve the balance between sensitivity and\n\nspecificity to get broad coverage of the literature and to avoid the unintentional\n\nomission of relevant articles (Siddaway et al. 2019).\n\nAs the first source of data, we used the social science citation index (SSCI) to\n\nensure broad coverage of scholarly literature. This database covers English-\n\nlanguage peer-reviewed journals in business and management. As part of the Web\n\nof Knowledge, the database includes all journals with an impact factor, which is a\n\nreasonable proxy for the most important publications in the field. We completed our\n\nsearch with the EBSCO Business Source Premier database to add further breadth.\n\nSince electronic databases are not fully comprehensive, we additionally searched in\n\nthe reference section of the considered papers and manually searched for articles\n\n(Siddaway et ",
      "text": [],
      "layoutText": []
    }
  ]
}

Query 3: Library query by organization search and filter to a specific author. 
{
  "search": "Amazon",
  "filter": "metadata_author eq 'Sandhya Narayanan '"
}

Response 3: 
{
  "@odata.context": "https://training-catalog-jms.search.windows.net/indexes('azureblob-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 3.8302023,
      "content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nNarayanan et al. J Big Data            (2020) 7:13  \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence:   \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n\n\n\n\nPage 5 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to  ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F  ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the  mth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using  L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\n\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T  of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24 months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope w",
      "metadata_storage_size": 1702203,
      "metadata_storage_path": "aHR0cHM6Ly9qdHJhaW5pbmdjYXRhbG9nc3RvcmFnZS5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL3M0MDUzNy0wMjAtMDAyOTIteS5wZGY1",
      "metadata_author": "Sandhya Narayanan ",
      "metadata_title": "Improving prediction with enhanced Distributed Memory-based Resilient Dataset Filter",
      "metadata_creation_date": "2020-02-24T16:27:45Z",
      "people": [
        "Sandhya Narayanan1",
        "Philip Samuel2",
        "Mariamma Chacko3",
        "Narayanan",
        "Makridakis",
        "15Narayanan",
        "Hao",
        "Salakhutdinov",
        "Wietsma",
        "Jianguo Chen",
        "Asha",
        "Luo",
        "Liu",
        "�G",
        "Iden"
      ],
      "organizations": [
        "ket",
        "Creative Commons",
        "Creative",
        "creat iveco \nmmons",
        "School of Engineering",
        "Cochin University of Science",
        "Cus",
        "positive",
        "LSA",
        "relational database management systems",
        "huge",
        "Sup",
        "Real",
        "f2",
        "fk",
        "ith",
        "Ef",
        "Resilient Distributed",
        "RDD",
        "DMRDF",
        "KCm",
        "KRi",
        "zi",
        "yi",
        "hyper plane",
        "rc",
        "ge",
        "TP",
        "TN",
        "FP",
        "FN",
        "Gini",
        "Classifier",
        "Gini-index",
        "performance evaluation",
        "prediction",
        "TP + FN",
        "SVM"
      ],
      "locations": [
        "shop",
        "India",
        "shopping sites",
        "un-structured",
        "Meth",
        "Gini",
        "Ri",
        "N",
        "path",
        "RDD",
        "KCm",
        "fi",
        "DMRDF",
        "Pe",
        "us",
        "PA",
        "TN",
        "FN",
        "LSA",
        "Ta",
        "Large"
      ],
      "keyphrases": [
        "Distributed Memory‑based Resilient Dataset Filter",
        "Creative Commons Attribution 4.0 International License",
        "Distributed Memory-based Resilient Dataset Filter",
        "other third party material",
        "big data processing technologies",
        "A Feature Information Gain",
        "other online shopping sites",
        "Resilient Distribution Dataset",
        "social networking sites",
        "Creative Commons licence",
        "sophisticated pre-processing techniques",
        "significant feature identification",
        "Support vector machine",
        "Redundancy Open Access",
        "J Big Data",
        "successful product launch",
        "online product recommendations",
        "consumer electronics market",
        "online product reviews",
        "distributed environment",
        "Online reviews",
        "pre-processed dataset",
        "feature modelling",
        "Online feedback",
        "retail shopping",
        "useful information",
        "price information",
        "author information",
        "Customer reviews",
        "duplicate reviews",
        "product sale",
        "product life",
        "product quality",
        "less product",
        "product pre-launch",
        "Sandhya Narayanan1",
        "Philip Samuel2",
        "Mariamma Chacko3",
        "massive volumes",
        "different applications",
        "sensor data",
        "health care",
        "enormous size",
        "unstructured data",
        "crucial thing",
        "large size",
        "communication methods",
        "direct suggestions",
        "several advantages",
        "limited time",
        "research work",
        "FIG) measure",
        "Logistic regression",
        "resilience property",
        "appropriate credit",
        "original author",
        "credit line",
        "statutory regulation",
        "copyright holder",
        "creat iveco",
        "RESEARCH Narayanan",
        "Cochin University",
        "Full list",
        "new product",
        "1 Information Technology",
        "intended use",
        "permitted use",
        "mation source",
        "DMRDF method",
        "The Author",
        "doi.org",
        "prediction accuracy",
        "Introduction",
        "Extracting",
        "amount",
        "extensive",
        "customers",
        "impact",
        "extent",
        "ratings",
        "Abstract",
        "sustainability",
        "companies",
        "turn",
        "reliability",
        "classifiers",
        "output",
        "manufacturer",
        "design",
        "Keywords",
        "article",
        "sharing",
        "adaptation",
        "reproduction",
        "medium",
        "link",
        "changes",
        "images",
        "permission",
        "Correspondence",
        "nairsands",
        "School",
        "Engineering",
        "Science",
        "Kochi",
        "India",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "different natural language processing techniques",
        "significant data processing methods",
        "big data processing model",
        "Different feature selection methods",
        "wrapper feature selection method",
        "new prod- uct",
        "multiple forecasting field",
        "previous customer feedbacks",
        "online shopping sites",
        "structured massive volume",
        "customer review analysis",
        "spam reviews recognition",
        "many redundant reviews",
        "machine learning methods",
        "Consumer product success",
        "product pre-launch prediction",
        "future work” section",
        "user rating matrix",
        "poor quality products",
        "different criteria",
        "Different works",
        "wrapper methods",
        "art methods",
        "alternative methods",
        "statistical methods",
        "model complexity",
        "statistical analysis",
        "filter method",
        "customer reviews",
        "enhanced method",
        "unreliable data",
        "model performance",
        "shop owners",
        "other hand",
        "extra cost",
        "brand loyalty",
        "eCommerce firms",
        "other retailers",
        "large volume",
        "crucial phase",
        "univariate manner",
        "System design",
        "less accuracy",
        "unknown values",
        "improper knowledge",
        "Matrix factorization",
        "collaborative filtering",
        "two vectors",
        "low dimensionality",
        "accurate reviews",
        "duplicated reviews",
        "negative reviews",
        "Related work",
        "product reviews",
        "odology” section",
        "data pre-processing",
        "marketing strategies",
        "embedded process",
        "prediction classifiers",
        "Page",
        "15Narayanan",
        "information",
        "effort",
        "industry",
        "strategy",
        "users",
        "valuable",
        "number",
        "blogs",
        "forums",
        "awareness",
        "need",
        "ratio",
        "positive",
        "features",
        "usefulness",
        "relevance",
        "state",
        "gener",
        "ally",
        "combination",
        "distributive",
        "web",
        "order",
        "scalable",
        "failure",
        "realization",
        "paper",
        "methodology",
        "Results",
        "discussions",
        "conclusion",
        "Makridakis",
        "Author",
        "reason",
        "MF",
        "Hao",
        "item",
        "user item matrix factorization technique",
        "standard probability-based matrix factorization methods",
        "user item matrix factorization method",
        "ity related matrix factorization",
        "Gini- index impurity measure",
        "relational database management systems",
        "single value decomposition method",
        "high term frequency words",
        "probability factorization methods",
        "student user behavior",
        "Stochastic Gradient Decent",
        "mobile decision aid",
        "rule-based apriori algorithm",
        "mobile envi- ronment",
        "appropriate computing models",
        "traditional collaborative Filtering",
        "product feature identification",
        "Bayesian-based probabilistic analysis",
        "cold start problem",
        "automatic service selection",
        "Latent Semantic Analysis",
        "Gini-index feature method",
        "movie review dataset",
        "customer review datasets",
        "pre-launch product prediction",
        "Statistical methods",
        "user reviews",
        "opinion words",
        "Gini-index method",
        "filtering function",
        "movie rating",
        "review summarization",
        "customer feedback",
        "density-peaked method",
        "LSA-based) method",
        "LSA-based method",
        "squared distance",
        "big volume",
        "conventional approach",
        "implementation purpose",
        "sparsity problem",
        "various analyses",
        "recommendation issues",
        "different websites",
        "Jianguo Chen",
        "opinion extraction",
        "individual dimensions",
        "large datasets",
        "major challenge",
        "existing products",
        "different classifiers",
        "rating dataset",
        "polarity prediction",
        "product features",
        "recommender system",
        "recommendation system",
        "huge volume",
        "historical data",
        "big data",
        "disease symptoms",
        "sentimental analysis",
        "diction accuracy",
        "29 features",
        "solution",
        "squares",
        "Salakhutdinov",
        "other",
        "items",
        "limitation",
        "addition",
        "approaches",
        "Wietsma",
        "study",
        "result",
        "correlation",
        "treatment",
        "diagnosis",
        "diseases",
        "cluster",
        "Asha",
        "sentences",
        "rence",
        "document",
        "precision",
        "disadvantage",
        "Luo",
        "quality",
        "Liu",
        "authors",
        "Lack",
        "redundancy",
        "work",
        "existence",
        "Methodology",
        "phases",
        "Distributed Memory-based Resilient Dataset Filter approach",
        "Identification Redundancy Removal Data Integration Training",
        "classification algorithms Support Vector Logistic",
        "Product prelaunch prediction System Design",
        "Data collection Categorical Text Real",
        "Enhanced Feature Information Gain measure",
        "Sup- port Vector Machine",
        "flip cart customer reviews",
        "ith customer review Ri",
        "Regression Support Vector",
        "duplicate data removal",
        "classification algorithms Logistic",
        "mobile phones product reviews",
        "data collection phase",
        "JSON file format",
        "New mobile phones",
        "product rating scale",
        "categorical, real",
        "Regression Testing Dataset",
        "Dataset pre‑processing",
        "product review dataset",
        "text data",
        "Logistic Regression",
        "opinion identification",
        "data pre",
        "best information",
        "prediction classifier",
        "multivariate data",
        "pre-launch prediction",
        "Product feature",
        "input dataset",
        "Product categories",
        "feature selection",
        "processing Feature",
        "nificant feature",
        "feature instance",
        "k feature",
        "various stages",
        "dancy elimination",
        "different products",
        "Several datasets",
        "public datasets",
        "data- set",
        "seven brands",
        "two reasons",
        "unavoidable items",
        "sample set",
        "catego- rization",
        "priority weightage",
        "major role",
        "large number",
        "fea- tures",
        "total number",
        "particular review",
        "user requirements",
        "feature impurity",
        "market industry",
        "Battery life",
        "Review Content",
        "user features",
        "Figure",
        "model",
        "SVM",
        "LR",
        "zon",
        "period",
        "24 months",
        "day",
        "everyone",
        "Table",
        "ReviewID",
        "Title",
        "price",
        "camera",
        "RAM",
        "Fig.",
        "processor",
        "Average",
        "polarity",
        "opinions",
        "Senti-WordNet",
        "probability",
        "fk",
        "SR",
        "Distributed Memory-based Resilience Dataset Filter",
        "15 Rear camera 31 Finger sensor",
        "big data processing approach",
        "launch predic- tion",
        "Resilient Distributed Dataset",
        "local file system",
        "3 ReviewID 19 Product category",
        "Impurity R(I",
        "value) pair dataset",
        "memory data caching",
        "information Gain value",
        "Feature Information Gain",
        "customer review dataset",
        "Table 2 Significant Features",
        "Ef Review Feature",
        "13 Operating system",
        "input file",
        "P(R",
        "25 Front camera",
        "new dataset",
        "prior information",
        "9 Feature information",
        "Next step",
        "OR N",
        "Sim type",
        "4 Content 20 Thickness",
        "mobile phone",
        "7 Battery life",
        "10 Review type",
        "29 Network support",
        "14 Water proof",
        "Quick charging",
        "32 Internal storage",
        "machine learning",
        "pervasive requirement",
        "main actions",
        "first element",
        "main Transformations",
        "long Lineage",
        "n customers",
        "feature set",
        "active customer",
        "5 Product brand",
        "Product type",
        "11 Product display",
        "redundant reviews",
        "N reviews",
        "value) pairs",
        "�G value",
        "null values",
        "Pi.log2Ef",
        "SR N",
        "cache chunks",
        "24 Product rating",
        "reduce function",
        "SR.",
        "respect",
        "opinion",
        "No",
        "1 Author",
        "17 RAM",
        "2 Title",
        "Weight",
        "8 Price",
        "12 Processor",
        "Multi-band",
        "16 Applications",
        "RDD",
        "requirements",
        "Variety",
        "jobs",
        "point",
        "time",
        "challenge",
        "analysis",
        "systems",
        "elements",
        "saveAsSequenceFile",
        "path",
        "map",
        "groupBykey",
        "ReduceBykey",
        "fault-tolerance",
        "list",
        "Fx",
        "∑",
        "β",
        "Distributed Memory-based resilient filter score",
        "hyper plane normal vector element",
        "memory-based Resilient Dataset Filter score",
        "D dimensional input space",
        "resilient filter score value",
        "Support Vector Machine classifiers",
        "t training feature vectors",
        "one Distributed Memory-based",
        "decision hyper plane",
        "positive one class",
        "logarithmic base value",
        "logistic regression analysis",
        "machine learning method",
        "prediction variable value",
        "Logistic regression value",
        "data features relationships",
        "product failure class",
        "product success class",
        "δ score value",
        "row vector",
        "column vector",
        "Prediction classifiers",
        "significant feature",
        "corresponding vectors",
        "nearest vectors",
        "learning approaches",
        "classification method",
        "training dataset",
        "constant value",
        "probability value",
        "mth customer",
        "L1 norm",
        "second occurrence",
        "case study",
        "mobile phones",
        "logit function",
        "+ b",
        "new skills",
        "data separation",
        "real numbers",
        "two classes",
        "mth review",
        "ith review",
        "customer review",
        "similar reviews",
        "successful products",
        "N’ number",
        "KC",
        "KR",
        "KFx",
        "KFj",
        "entry",
        "similarities",
        "The",
        "Eq.",
        "processing",
        "More",
        "market",
        "p0",
        "L0",
        "values",
        "knowledge",
        "RD",
        "XD",
        "way",
        "distance",
        "hyperplane",
        "conditions",
        "margin",
        "γ",
        "different Spark cluster configura- tions",
        "Most significant customer review features",
        "two Intel Xeon E",
        "2699V4 2.2 G Hz processors",
        "Web Server Gateway Interface",
        "customer review feature identification",
        "big data processing system",
        "software system large servers",
        "LSA-based methods processing time",
        "big data analytics",
        "Apache web server",
        "Amazon Web Services",
        "Apache Spark 2.2.1 framework",
        "Logistic regression classifiers",
        "system perfor- mance",
        "different dataset size",
        "feature information gain",
        "negative one class",
        "semantic analysis methods",
        "high-speed processing performance",
        "system response time",
        "Spark python API",
        "several case studies",
        "system design factors",
        "mobile phone sustainability",
        "prediction accuracy measurement",
        "redundant customer reviews",
        "prediction accuracy evaluation",
        "DMRDF model time",
        "proposed system",
        "separate servers",
        "prediction system",
        "failure class",
        "software components",
        "LSA-based model",
        "less time",
        "product sustainability",
        "Experimental setup",
        "PySpark version",
        "Vector Machine",
        "predic- tion",
        "major concern",
        "internal storage",
        "art techniques",
        "DMRDF approach",
        "9 GB dataset",
        "18 GB dataset",
        "DMRDF model 740",
        "other gini-index",
        "Product price",
        "scalability requirements",
        "other state",
        "Gini-index model",
        "application development",
        "16 GB",
        "gramming",
        "Ubuntu",
        "nodes",
        "VCPUs",
        "4 cores",
        "wtzi",
        "Support",
        "7 brands",
        "LR.",
        "graph",
        "percentage",
        "manner",
        "comparison",
        "completion",
        "latent",
        "342 s",
        "495 s",
        "156 s",
        "910 s",
        "advantage",
        "execution",
        "recall",
        "Distributed Memory-based Resilient Dataset Filter method",
        "reliable big data processing model",
        "Support Vector Machine prediction classifiers",
        "Support Vector Machine classification",
        "Classifier Support vector machine",
        "different customer review aspects",
        "customer review feature prediction",
        "Processing Time Graph",
        "big data analysis",
        "Months LSA-based DMRDF Gini-index",
        "LSA-based meth- ods",
        "Logistic Regression classifiers",
        "other two methods",
        "duplicate customer reviews",
        "Classifier Logistic regression",
        "LR classifiers",
        "redundant data",
        "feature dimensionality",
        "other methods",
        "Gini-index methods",
        "Dataset size",
        "LSA-based approaches",
        "Gini-index approaches",
        "significant features",
        "accuracy measures",
        "P@R",
        "R@R",
        "false negative",
        "1GB 5GB",
        "SVM classifier",
        "ratings datasets",
        "performance evaluation",
        "Gini- index",
        "Technological development",
        "new challenges",
        "artificial intelligence",
        "next frontier",
        "mation Gain",
        "The DMRDF",
        "large dataset",
        "many features",
        "Performance comparison",
        "future work",
        "PA measures",
        "results",
        "TP",
        "FP",
        "TN",
        "FN",
        "Eqs",
        "18GB",
        "tions",
        "Conclusion",
        "era",
        "innovation",
        "productivity",
        "implementation",
        "elimination",
        "12",
        "7",
        "other product feature identification",
        "different reliable online websites",
        "prediction model- ling",
        "data processing domains",
        "information fusion approach",
        "statistical prop- erties",
        "memory computation method",
        "time streaming predictions",
        "dataset model performance",
        "important role",
        "DMRDF model",
        "applica- tion",
        "Resilience property",
        "long lineage",
        "Proposed design",
        "unified API",
        "customer comments",
        "learning algorithms",
        "real",
        "surveys",
        "thesis",
        "sentiments",
        "machine"
      ],
      "merged_content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nNarayanan et al. J Big Data            (2020) 7:13  \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence:   \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n  \n\n\n\nPage 5 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to  ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F  ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the  mth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using  L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\n\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T  of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24 months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope w",
      "text": [
        "",
        "Published online: 28 February 2020"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"Published online: 28 February 2020\",\"lines\":[{\"boundingBox\":[{\"x\":4,\"y\":16},{\"x\":1018,\"y\":18},{\"x\":1018,\"y\":73},{\"x\":4,\"y\":70}],\"text\":\"Published online: 28 February 2020\"}],\"words\":[{\"boundingBox\":[{\"x\":5,\"y\":16},{\"x\":269,\"y\":17},{\"x\":270,\"y\":70},{\"x\":5,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":291,\"y\":17},{\"x\":498,\"y\":17},{\"x\":498,\"y\":71},{\"x\":292,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":508,\"y\":17},{\"x\":575,\"y\":17},{\"x\":575,\"y\":71},{\"x\":508,\"y\":71}],\"text\":\"28\"},{\"boundingBox\":[{\"x\":593,\"y\":17},{\"x\":852,\"y\":18},{\"x\":852,\"y\":73},{\"x\":594,\"y\":71}],\"text\":\"February\"},{\"boundingBox\":[{\"x\":871,\"y\":18},{\"x\":1008,\"y\":18},{\"x\":1008,\"y\":74},{\"x\":871,\"y\":73}],\"text\":\"2020\"}]}"
      ]
    }
  ]
}

Query 4: Courses show facets for source, role, and level
{
  "search": "*",
  "facets": ["source", "role", "level"]
}

Response 4: 
{
  "@odata.context": "https://training-catalog-jms.search.windows.net/indexes('azuretable-index')/$metadata#docs(*)",
  "@search.facets": {
    "level": [
      {
        "count": 635,
        "value": "beginner"
      },
      {
        "count": 327,
        "value": "intermediate"
      },
      {
        "count": 28,
        "value": "advanced"
      },
      {
        "count": 3,
        "value": "Intermediate"
      },
      {
        "count": 1,
        "value": "Advanced"
      },
      {
        "count": 1,
        "value": "Beginner"
      }
    ],
    "role": [
      {
        "count": 174,
        "value": "developer"
      },
      {
        "count": 130,
        "value": "solution-architect"
      },
      {
        "count": 108,
        "value": "business-user"
      },
      {
        "count": 106,
        "value": "devops-engineer"
      },
      {
        "count": 105,
        "value": "administrator"
      },
      {
        "count": 75,
        "value": "functional-consultant"
      },
      {
        "count": 56,
        "value": "data-scientist"
      },
      {
        "count": 56,
        "value": "maker"
      },
      {
        "count": 56,
        "value": "student"
      },
      {
        "count": 47,
        "value": "ai-engineer"
      }
    ],
    "source": [
      {
        "count": 965,
        "value": "MS Learn"
      },
      {
        "count": 16,
        "value": "Udacity"
      },
      {
        "count": 14,
        "value": "Company Moodle"
      }
    ]
  },
  "@search.nextPageParameters": {
    "search": "*",
    "facets": [
      "source",
      "role",
      "level"
    ],
    "skip": 50
  },
  "value": [
    {
      "@search.score": 1,
      "RowKey": "17b1eedc-0e96-4e5b-8199-83a484388efe",
      "description": "Learn our internal best practices for using the O365 suite including email signatures, file storage and other issues",
      "duration": "2",
      "instructor": "Gerald Dominguez",
      "rating_average": 4.6,
      "rating_count": 510,
      "role": "all",
      "source": "Company Moodle",
      "url": "https://www.example.com/course10",
      "title": "O365",
      "product": "O365",
      "level": "beginner",
      "keyphrases": [
        "company",
        "moodle"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "278d299e-ef0e-47fb-8e98-5a31a073519c",
      "description": "For developers, learn our best practices for securely connecting to databases",
      "duration": "2",
      "instructor": "Eileen Diaz",
      "rating_average": 4.8,
      "rating_count": 115,
      "role": "developer",
      "source": "Company Moodle",
      "url": "https://www.example.com/course7",
      "title": "Security for database code",
      "product": "SQL",
      "level": "advanced",
      "keyphrases": [
        "company",
        "moodle"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "30e3c6e5-9415-4d85-8229-c2133203c535",
      "description": "Learn the policies related to the distribution and use of computers, phones, software, and other technology",
      "duration": "1",
      "instructor": "Mike Montoya",
      "rating_average": 4.9,
      "rating_count": 550,
      "role": "all",
      "source": "Company Moodle",
      "url": "https://www.example.com/course2",
      "title": "Onboarding - Technology Policies",
      "product": "NA",
      "level": "beginner",
      "keyphrases": [
        "company",
        "moodle"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "3cbb3800-2554-4121-bb3a-f365deb0c3b6",
      "description": "Learn our best practices for various tools such as Leaflet",
      "duration": "2",
      "instructor": "Robert Gillis",
      "rating_average": 3.9,
      "rating_count": 28,
      "role": "developer",
      "source": "Company Moodle",
      "url": "https://www.example.com/course6",
      "title": "Maps",
      "product": "leaflet",
      "level": "intermediate",
      "keyphrases": [
        "company",
        "moodle"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "578a3319-aa7c-4d2f-b6a4-39e9638b0a85",
      "description": "For administrators, this course will teach you how our CI/CD pipelines work from an operations perspective",
      "duration": "5",
      "instructor": "Claudia Blackman",
      "rating_average": 4.9,
      "rating_count": 56,
      "role": "admin",
      "source": "Company Moodle",
      "url": "https://www.example.com/course5",
      "title": "DevOps for Ops",
      "product": "jenkins",
      "level": "intermediate",
      "keyphrases": [
        "company",
        "moodle"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "5b293283-81e6-4f89-a0ab-7053988d6f6a",
      "description": "Learn how to track billable and non-billable hours by assigning time to projects and other relevant time codes",
      "duration": "1",
      "instructor": "Mike Montoya",
      "rating_average": 4.8,
      "rating_count": 540,
      "role": "all",
      "source": "Company Moodle",
      "url": "https://www.example.com/course1",
      "title": "Onboarding - Time Tracking",
      "product": "NA",
      "level": "beginner",
      "keyphrases": [
        "company",
        "moodle"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "6b5d3f55-eb02-499d-9775-2e0e25659e07",
      "description": "Learn our company's Principles for the Responsible Use of AI",
      "duration": "1",
      "instructor": "Eileen Diaz",
      "rating_average": 4.3,
      "rating_count": 24,
      "role": "architect",
      "source": "Company Moodle",
      "url": "https://www.example.com/course12",
      "title": "Ethics in AI",
      "product": "NA",
      "level": "intermediate",
      "keyphrases": [
        "company",
        "moodle"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "85ee725b-4ae0-4719-8785-ddf99e19faf1",
      "description": "For administrators, learn our best practices for securing all databases",
      "duration": "3",
      "instructor": "Eileen Diaz",
      "rating_average": 4.3,
      "rating_count": 45,
      "role": "admin",
      "source": "Company Moodle",
      "url": "https://www.example.com/course8",
      "title": "Security for database admins",
      "product": "SQL",
      "level": "advanced",
      "keyphrases": [
        "company",
        "moodle"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "9700e1dc-b293-4306-9e1b-0d345863db54",
      "description": "This course will teach you the specific ways our company uses Git. You will learn details for comments, branching, pull requests, and other procsses",
      "duration": "3",
      "instructor": "Claudia Blackman",
      "rating_average": 4.5,
      "rating_count": 125,
      "role": "developer",
      "source": "Company Moodle",
      "url": "https://www.example.com/course3",
      "title": "Git Workflow",
      "product": "git",
      "level": "beginner",
      "keyphrases": [
        "company",
        "moodle"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "9df844fd-eefe-4880-8341-936732174bb5",
      "description": "Learn our policies for utilizing encryption including key management for projects",
      "duration": "3",
      "instructor": "Eileen Diaz",
      "rating_average": 4.2,
      "rating_count": 95,
      "role": "architect",
      "source": "Company Moodle",
      "url": "https://www.example.com/course14",
      "title": "Encryption and security",
      "product": "NA",
      "level": "advanced",
      "keyphrases": [
        "company",
        "moodle"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "b51ede14-025f-49ad-9e9e-44ad284eedda",
      "description": "For developers, this course will teach you how to hook your dev work into our existing CI/CD pipelines.",
      "duration": "3",
      "instructor": "Claudia Blackman",
      "rating_average": 3.8,
      "rating_count": 101,
      "role": "developer",
      "source": "Company Moodle",
      "url": "https://www.example.com/course4",
      "title": "DevOps for Dev",
      "product": "jenkins",
      "level": "intermediate",
      "keyphrases": [
        "company",
        "moodle"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "c014cf36-2a83-4ded-bfe5-d62f405ba970",
      "description": "This course will teach you best practices for communicating with your team while working remotely",
      "duration": "1",
      "instructor": "Gerald Dominguez",
      "rating_average": 4.7,
      "rating_count": 325,
      "role": "all",
      "source": "Company Moodle",
      "url": "https://www.example.com/course11",
      "title": "Remote work",
      "product": "NA",
      "level": "beginner",
      "keyphrases": [
        "company",
        "moodle"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "c6b7fbd0-7390-4370-ab77-45027596b520",
      "description": "Understand ways you can be more healthy in the work environment including what ergonomic equipment is available to you",
      "duration": "1",
      "instructor": "Mike Montoya",
      "rating_average": 4.6,
      "rating_count": 525,
      "role": "all",
      "source": "Company Moodle",
      "url": "https://www.example.com/course13",
      "title": "Workplace Health",
      "product": "NA",
      "level": "beginner",
      "keyphrases": [
        "company",
        "moodle"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "d3f0c955-ac6e-4ced-b91b-ffcef3e8cede",
      "description": "For developers, learn our best practices for writing secure code for web, server, and desktop development",
      "duration": "3",
      "instructor": "Eileen Diaz",
      "rating_average": 4.4,
      "rating_count": 132,
      "role": "developer",
      "source": "Company Moodle",
      "url": "https://www.example.com/course9",
      "title": "Code security",
      "product": "NA",
      "level": "intermediate",
      "keyphrases": [
        "company",
        "moodle"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "000a0d57-a0fe-4386-829c-99074d1b3b9b",
      "description": "Find out about automated testing that proves your code to be maintainable, understandable, and functioning without repetitive manual testing.",
      "duration": "82",
      "instructor": null,
      "rating_average": 4.73,
      "rating_count": 3301,
      "role": "solution-architect",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/run-quality-tests-build-pipeline/?WT.mc_id=api_CatalogApi",
      "title": "Run quality tests in your build pipeline by using Azure Pipelines",
      "product": "azure-devops",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "002f4436-7360-4daa-a21d-f9dcd3518589",
      "description": "Enable business users with key AI use cases",
      "duration": "34",
      "instructor": null,
      "rating_average": 4.75,
      "rating_count": 758,
      "role": "functional-consultant",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/enable-business-users-with-key-ai-uses-cases/?WT.mc_id=api_CatalogApi",
      "title": "Enable business users with key AI use cases",
      "product": "power-platform",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "00baaa75-89fe-4f86-805f-f08336e6af48",
      "description": "Explore the strategic components, use cases, and special factors of an enterprise AI strategy that creates real business value, with INSEAD and Microsoft.",
      "duration": "70",
      "instructor": null,
      "rating_average": 4.71,
      "rating_count": 2779,
      "role": "business-user",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/ai-strategy-to-create-business-value/?WT.mc_id=api_CatalogApi",
      "title": "Define an AI strategy to create business value",
      "product": "m365",
      "level": "intermediate",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "015c2947-8e4e-44f5-b342-0564f0ee2fbf",
      "description": "Learn to manage LUIS apps through versioning, key management, handling data, and improving predictions.",
      "duration": "24",
      "instructor": null,
      "rating_average": 4.77,
      "rating_count": 106,
      "role": "developer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/manage-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "title": "Manage your Language Understanding Intelligent Service (LUIS) Apps",
      "product": "azure",
      "level": "intermediate",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "01731c10-20bb-41e7-ba21-8528669dcdc3",
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "duration": "18",
      "instructor": null,
      "rating_average": 4.75,
      "rating_count": 137,
      "role": "ai-engineer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "product": "azure",
      "level": "advanced",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "0197c8c6-dfc4-450b-9fa7-3f610977cc79",
      "description": "Learn about AI Builder Text recognition and how to use it with other Power Platform products.",
      "duration": "55",
      "instructor": null,
      "rating_average": 4.61,
      "rating_count": 197,
      "role": "business-user",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-text-recognition/?WT.mc_id=api_CatalogApi",
      "title": "Get started with AI Builder Text recognition",
      "product": "ai-builder",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "01b1e42d-8025-4865-8488-06e946fd5bf3",
      "description": "In this module, we'll introduce you to Language Understanding Intelligent Service (LUIS) and show how to build and publish a LUIS model",
      "duration": "54",
      "instructor": null,
      "rating_average": 4.67,
      "rating_count": 1855,
      "role": "ai-engineer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-and-publish-a-luis-model/?WT.mc_id=api_CatalogApi",
      "title": "Add conversational intelligence to your apps by using Language Understanding Intelligent Service (LUIS)",
      "product": "azure-bot-service",
      "level": "intermediate",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "01bf705b-5798-4adb-ba5c-e5c018e6e9c2",
      "description": "Learn to identify valuable information in conversations with LUIS for interpreting user goals (intents) and distill valuable information from sentences (entities).",
      "duration": "27",
      "instructor": null,
      "rating_average": 4.77,
      "rating_count": 131,
      "role": "solution-architect",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/add-basic-conversational-intelligence/?WT.mc_id=api_CatalogApi",
      "title": "Add basic conversational intelligence to your apps by using Language Understanding Intelligent Service (LUIS)",
      "product": "azure-cognitive-services",
      "level": "intermediate",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "01c3e802-467a-4fa6-828d-5e7e5a6a5d21",
      "description": "Explore two capabilities in the DevOps taxonomy, Continuous Collaboration and Continuous Improvement.",
      "duration": "17",
      "instructor": null,
      "rating_average": 4.7,
      "rating_count": 1425,
      "role": "developer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/characterize-devops-continous-collaboration-improvement/?WT.mc_id=api_CatalogApi",
      "title": "Characterize DevOps Continuous Collaboration and Continuous Improvement",
      "product": "azure",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "021d4ac7-4b99-45de-9d6f-972581d73c46",
      "description": "Learn to identify valuable information in conversations with LUIS for interpreting user goals (intents) and distill valuable information from sentences (entities).",
      "duration": "27",
      "instructor": null,
      "rating_average": 4.77,
      "rating_count": 131,
      "role": "developer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/add-basic-conversational-intelligence/?WT.mc_id=api_CatalogApi",
      "title": "Add basic conversational intelligence to your apps by using Language Understanding Intelligent Service (LUIS)",
      "product": "azure-language-understanding",
      "level": "intermediate",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "025d928d-026f-4bc9-bff6-f7f3229667c1",
      "description": "Implement CI/CD pipeline for Node.js applications. Automate deployment of Vue, React, Angular or webpack apps with Azure Pipelines.",
      "duration": "44",
      "instructor": null,
      "rating_average": 4.9,
      "rating_count": 41,
      "role": "solution-architect",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/deploy-nodejs/?WT.mc_id=api_CatalogApi",
      "title": "Automate Node.js deployments with Azure Pipelines",
      "product": "azure",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "029119c3-cd3c-4ac3-bda6-06fe364b9a54",
      "description": "Choose and implement a deployment pattern that helps you smoothly roll out new application features to your users.",
      "duration": "60",
      "instructor": null,
      "rating_average": 4.72,
      "rating_count": 604,
      "role": "administrator",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/manage-release-cadence/?WT.mc_id=api_CatalogApi",
      "title": "Manage release cadence in Azure Pipelines by using deployment patterns",
      "product": "azure-devops",
      "level": "intermediate",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "02a0e579-5c88-4fbf-9307-5760677a6efb",
      "description": "Learn how to manage changes to your repository source by using pull requests.",
      "duration": "49",
      "instructor": null,
      "rating_average": 4.75,
      "rating_count": 292,
      "role": "developer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/manage-changes-pull-requests-github/?WT.mc_id=api_CatalogApi",
      "title": "Manage repository changes by using pull requests on GitHub",
      "product": "azure",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "033ed430-5445-4f84-9c31-2a20126b682c",
      "description": "Learn how to contribute to an open-source project on GitHub.",
      "duration": "27",
      "instructor": null,
      "rating_average": 4.88,
      "rating_count": 59,
      "role": "solution-architect",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/contribute-open-source/?WT.mc_id=api_CatalogApi",
      "title": "Contribute to an open-source project on GitHub",
      "product": "vs",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "03b72eb0-78e3-44eb-83b8-3d4f35d541a1",
      "description": "Implement robotic process automation with Microsoft Power Automate, Teams, desktop flow, and AI Builder",
      "duration": "34",
      "instructor": null,
      "rating_average": 4.73,
      "rating_count": 721,
      "role": "maker",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/implement-power-automate-ui-flows-ai-builder/?WT.mc_id=api_CatalogApi",
      "title": "Implement robotic process automation with Microsoft Power Automate, Teams, desktop flow, and AI Builder",
      "product": "power-platform",
      "level": "intermediate",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "04092659-deab-43ad-ad0e-ef2992f0af29",
      "description": "In this module, we'll authenticate to GitHub, create a local Git repository, and push the repository to GitHub by using the Git tooling experience in Visual Studio 2019. We'll add and modify files, stage and commit changes, and then finally push to your remote.",
      "duration": "38",
      "instructor": null,
      "rating_average": 5,
      "rating_count": 8,
      "role": "administrator",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/visual-studio-github-push/?WT.mc_id=api_CatalogApi",
      "title": "Get started with Git and GitHub in Visual Studio",
      "product": "vs",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "0479d887-32ce-477b-b184-9230e096562a",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "data-scientist",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "title": "Get started with AI on Azure",
      "product": "azure-machine-learning",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "047b3781-308a-436d-af35-dbeb8f1b05cb",
      "description": "Learn to use the GitHub integration in Visual Studio Code, including authentication, publishing repos, and viewing your repo timeline.",
      "duration": "22",
      "instructor": null,
      "rating_average": 4.82,
      "rating_count": 73,
      "role": "developer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/introduction-to-github-visual-studio-code/?WT.mc_id=api_CatalogApi",
      "title": "Introduction to GitHub in Visual Studio Code",
      "product": "github",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "048934af-02e4-43b8-bd22-48b6a114efdb",
      "description": "Use Python, Flask, and Azure Cognitive Services to build a web app that incorporates AI",
      "duration": "75",
      "instructor": null,
      "rating_average": 4.65,
      "rating_count": 248,
      "role": "developer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/python-flask-build-ai-web-app/?WT.mc_id=api_CatalogApi",
      "title": "Build an AI web app by using Python and Flask",
      "product": "azure",
      "level": "intermediate",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "04e41100-803a-4534-b5fa-b4ac370d2d06",
      "description": "Implement a CI/CD pipeline for multiple containers to Kubernetes.",
      "duration": "56",
      "instructor": null,
      "rating_average": 4.7,
      "rating_count": 401,
      "role": "devops-engineer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/deploy-kubernetes/?WT.mc_id=api_CatalogApi",
      "title": "Automate multi-container Kubernetes deployments with Azure Pipelines",
      "product": "azure",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "04fb69f0-8201-4d9e-a8ac-2fa30de7bff5",
      "description": "DevOps is the union of people, process, and products to enable continuous delivery of value to our end users. Discover the first two foundation pillars of DevOps: Culture and Lean Product.",
      "duration": "25",
      "instructor": null,
      "rating_average": 4.69,
      "rating_count": 3395,
      "role": "business-user",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/introduce-foundation-pillars-devops/?WT.mc_id=api_CatalogApi",
      "title": "Introduce the foundation pillars of DevOps: Culture and Lean Product",
      "product": "azure",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "05589f26-6e80-4842-9f41-afb38aaa5add",
      "description": "Run Selenium UI tests, a form of functional testing, in Azure Pipelines.",
      "duration": "63",
      "instructor": null,
      "rating_average": 4.66,
      "rating_count": 800,
      "role": "administrator",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/run-functional-tests-azure-pipelines/?WT.mc_id=api_CatalogApi",
      "title": "Run functional tests in Azure Pipelines",
      "product": "azure",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "057dd8cb-9502-4710-acf1-3c5d76b3addf",
      "description": "Implement a CI/CD pipeline for Python.",
      "duration": "46",
      "instructor": null,
      "rating_average": 4.78,
      "rating_count": 144,
      "role": "devops-engineer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/deploy-python/?WT.mc_id=api_CatalogApi",
      "title": "Automate Python deployments with Azure Pipelines",
      "product": "azure-devops",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "058f6f58-26cc-4826-b80c-82eeb1471d46",
      "description": "Learn to manage a successful InnerSource program on GitHub through effective discoverability, guidance, and maintenance.",
      "duration": "50",
      "instructor": null,
      "rating_average": 4.75,
      "rating_count": 247,
      "role": "devops-engineer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/manage-innersource-program-github/?WT.mc_id=api_CatalogApi",
      "title": "Manage an InnerSource program by using GitHub",
      "product": "github",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "05cc1221-41c5-4da0-969f-00d4386dc9bf",
      "description": "Understand the key elements that make a culture AI-ready and a framework to drive the change needed in your organization for a successful AI strategy.",
      "duration": "49",
      "instructor": null,
      "rating_average": 4.74,
      "rating_count": 1136,
      "role": "functional-consultant",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/build-an-ai-ready-culture/?WT.mc_id=api_CatalogApi",
      "title": "Understand the importance of building an AI-ready culture",
      "product": "dynamics-365",
      "level": "intermediate",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "060cc647-783c-475d-8a83-5dcaff12ee84",
      "description": "Analyze images with the Computer Vision service",
      "duration": "28",
      "instructor": null,
      "rating_average": 4.73,
      "rating_count": 1283,
      "role": "ai-engineer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/analyze-images-computer-vision/?WT.mc_id=api_CatalogApi",
      "title": "Analyze images with the Computer Vision service",
      "product": "azure-cognitive-services",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "06695551-a3b3-497b-9f4e-7590158eb3e0",
      "description": "Explore two capabilities in the DevOps taxonomy, Continuous Collaboration and Continuous Improvement.",
      "duration": "17",
      "instructor": null,
      "rating_average": 4.7,
      "rating_count": 1425,
      "role": "business-user",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/characterize-devops-continous-collaboration-improvement/?WT.mc_id=api_CatalogApi",
      "title": "Characterize DevOps Continuous Collaboration and Continuous Improvement",
      "product": "azure",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "07291594-4cc9-4973-b36a-18f82767ddac",
      "description": "Do you want to know how to configure and set up an automated build process? In this module, you'll learn how to configure CI/CD with Business Central applications, how to create pipelines using YAML files, and set up a release process.",
      "duration": "43",
      "instructor": null,
      "rating_average": 4.62,
      "rating_count": 13,
      "role": "developer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-azure-pipelines/?WT.mc_id=api_CatalogApi",
      "title": "Use Azure Pipelines for CI/CD with Business Central",
      "product": "dynamics-365",
      "level": "intermediate",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "073da328-b0ac-4a34-bed6-62b6c1cbc8ad",
      "description": "Create a basic GitHub Action and use that action in a workflow.",
      "duration": "49",
      "instructor": null,
      "rating_average": 4.71,
      "rating_count": 254,
      "role": "administrator",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/github-actions-automate-tasks/?WT.mc_id=api_CatalogApi",
      "title": "Automate development tasks by using GitHub Actions",
      "product": "azure",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "07de0d2d-ff7e-4486-9a53-2a82b0fd737e",
      "description": "Detect and track polar bears through photos using AI, and then use Power BI to show where cameras spot polar bears.",
      "duration": "82",
      "instructor": null,
      "rating_average": 4.68,
      "rating_count": 318,
      "role": "data-scientist",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/build-ml-model-with-azure-stream-analytics/?WT.mc_id=api_CatalogApi",
      "title": "Track wild polar bears with AI",
      "product": "power-bi",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "081e4b03-eac1-48d8-aca0-d9a98e56b787",
      "description": "Learn how to contribute to an open-source project on GitHub.",
      "duration": "27",
      "instructor": null,
      "rating_average": 4.88,
      "rating_count": 59,
      "role": "administrator",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/contribute-open-source/?WT.mc_id=api_CatalogApi",
      "title": "Contribute to an open-source project on GitHub",
      "product": "vs",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "0859601c-73bb-4095-ac50-9489222f0790",
      "description": "Publish automatically and securely your code libraries or Docker images with GitHub Packages.",
      "duration": "38",
      "instructor": null,
      "rating_average": 4.71,
      "rating_count": 51,
      "role": "developer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/github-actions-packages/?WT.mc_id=api_CatalogApi",
      "title": "Leverage GitHub Actions to publish to GitHub Packages",
      "product": "github",
      "level": "intermediate",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "0862f7ef-cd02-48bb-925e-56b66a01d61d",
      "description": "Use a release approval in Azure Pipelines to help coordinate database schema changes between developers and database administrators.",
      "duration": "74",
      "instructor": null,
      "rating_average": 4.65,
      "rating_count": 714,
      "role": "solution-architect",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/manage-database-changes-in-azure-pipelines/?WT.mc_id=api_CatalogApi",
      "title": "Manage database changes in Azure Pipelines",
      "product": "azure-devops",
      "level": "intermediate",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "086d20fa-85e6-4e95-a6fe-e95b525af326",
      "description": "Explore the first two capabilities in the DevOps taxonomy, Continuous Planning and Continuous Integration.",
      "duration": "22",
      "instructor": null,
      "rating_average": 4.73,
      "rating_count": 2061,
      "role": "developer",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/analyze-devops-continuous-planning-intergration/?WT.mc_id=api_CatalogApi",
      "title": "Analyze DevOps Continuous Planning and Continuous Integration",
      "product": "azure",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "090e8f66-56ea-46c6-913b-c75b14113a31",
      "description": "Explore how to use GitHub Actions to create an automated Azure Kubernetes Service deployment pipeline.",
      "duration": "80",
      "instructor": null,
      "rating_average": 4.5,
      "rating_count": 36,
      "role": "solution-architect",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/aks-deployment-pipeline-github-actions/?WT.mc_id=api_CatalogApi",
      "title": "Azure Kubernetes Service deployment pipeline and GitHub Actions",
      "product": "github",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "0962bb4a-7b2e-4c38-a0b4-49b782a62e4e",
      "description": "Learn Microsoft guidelines for the development of responsible conversational AI, such as chat bots and voice-controlled systems.",
      "duration": "39",
      "instructor": null,
      "rating_average": 4.82,
      "rating_count": 33,
      "role": "business-user",
      "source": "MS Learn",
      "url": "https://docs.microsoft.com/en-us/learn/modules/responsible-conversational-ai/?WT.mc_id=api_CatalogApi",
      "title": "Discover Microsoft guidelines for responsible conversational AI development",
      "product": "dynamics-365",
      "level": "beginner",
      "keyphrases": [
        "ms",
        "learn"
      ]
    }
  ],
  "@odata.nextLink": "https://training-catalog-jms.search.windows.net/indexes/azuretable-index/docs/search?api-version=2024-05-01-preview"
}

Query 5: Courses filtered by length
{
  "search": "*",
  "facets": ["source", "role", "level"],
  "filter": "duration eq '3 months'"
}

Response 5: 
{
  "@odata.context": "https://training-catalog-jms.search.windows.net/indexes('azuretable-index')/$metadata#docs(*)",
  "@search.facets": {
    "level": [
      {
        "count": 2,
        "value": "intermediate"
      },
      {
        "count": 1,
        "value": "Intermediate"
      },
      {
        "count": 1,
        "value": "beginner"
      }
    ],
    "role": [
      {
        "count": 1,
        "value": "AI engineer"
      },
      {
        "count": 1,
        "value": "Machine Learning Engineer"
      },
      {
        "count": 1,
        "value": "data product manager"
      },
      {
        "count": 1,
        "value": "engineer"
      }
    ],
    "source": [
      {
        "count": 4,
        "value": "Udacity"
      }
    ]
  },
  "value": [
    {
      "@search.score": 1,
      "RowKey": "851bb622-77b5-488a-8c91-e4fbfe484852",
      "description": "This program will teach you how to become a better Artificial Intelligence or Machine Learning Engineer by teaching you classical AI algorithms applied to common problem types. You will complete projects and exercises incorporating search, optimization, planning, and probabilistic graphical models which have been used in Artificial Intelligence applications for automation, logistics, operations research, and more. These concepts form the foundation for many of the most exciting advances in AI in recent years. Each project you build will be an opportunity to demonstrate what you’ve learned in your lessons, and become part of a career portfolio that will demonstrate your mastery of these skills to potential employers",
      "duration": "3 months",
      "instructor": null,
      "rating_average": 4.5,
      "rating_count": 250,
      "role": "AI engineer",
      "source": "Udacity",
      "url": "https://www.udacity.com/course/ai-artificial-intelligence-nanodegree--nd898",
      "title": "Artificial Intelligence",
      "product": "Python",
      "level": "intermediate",
      "keyphrases": [
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "b2d2ad39-10f3-4513-90a5-f1b3d9bf430a",
      "description": "This program focuses on the fundamental building blocks you will need to learn in order to become an AI practitioner. Specifically, you will learn programming skills, and essential math for building an AI architecture. You’ll even dive into neural networks and deep learning. One of our main goals at Udacity is to help you create a job-ready portfolio. Building a project is one of the best ways to test the skills you’ve acquired, and to demonstrate your newfound abilities to prospective employers. In this Nanodegree program you will test your ability to use a pre-trained neural network architecture, and also have the opportunity to prove your skills by building your own image classifier. In the sections below, you’ll find detailed descriptions of the projects, along with the course material that presents the skills required to complete them.",
      "duration": "3 months",
      "instructor": null,
      "rating_average": 4.6,
      "rating_count": 800,
      "role": "engineer",
      "source": "Udacity",
      "url": "https://www.udacity.com/course/ai-programming-python-nanodegree--nd089",
      "title": "AI Programming with Python",
      "product": "Python",
      "level": "beginner",
      "keyphrases": [
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "d0d366b2-71cb-47ef-9257-fc2f05fd0c4d",
      "description": "In this program, students will enhance their skills by building and deploying sophisticated Machine Learning (ML)             solutions using popular open source tools and frameworks such as scikit-learn. Using Azure Machine Learning’s MLOps capabilities, students will gain experience in understanding their ML models, protecting people and their data, and controlling the end-to-end ML lifecycle at scale. Gain practical experience by using the built-in Azure labs accessible inside the Udacity classroom and run complex machine learning tasks for no additional cost. ",
      "duration": "3 months",
      "instructor": null,
      "rating_average": null,
      "rating_count": null,
      "role": "Machine Learning Engineer",
      "source": "Udacity",
      "url": "https://www.udacity.com/course/machine-learning-engineer-for-microsoft-azure-nanodegree--nd00333",
      "title": "Machine Learning Engineer for Microsoft Azure",
      "product": "Python, Azure ML SDK, Hyperdrive, AutoML",
      "level": "intermediate",
      "keyphrases": [
        "learn"
      ]
    },
    {
      "@search.score": 1,
      "RowKey": "fc5a8832-24a3-4d75-a2fc-4f90c41d2831",
      "description": "Leverage market data to amplify product development. Learn how to apply data science techniques, data engineering processes, and market experimentation tests to deliver customized product experiences. Begin by leveraging the power of SQL and Tableau to inform product strategy. Then, develop data pipelines and warehousing strategies that prepare data collected from a product for robust analysis. Finally, learn techniques for evaluating the data from live products, including how to design and execute various A/B and multivariate tests to shape the next iteration of a product.",
      "duration": "3 months",
      "instructor": null,
      "rating_average": null,
      "rating_count": null,
      "role": "data product manager",
      "source": "Udacity",
      "url": "https://www.udacity.com/course/data-product-manager-nanodegree--nd030",
      "title": "Data Product Manager",
      "product": "SQL, Tableau",
      "level": "Intermediate",
      "keyphrases": [
        "learn"
      ]
    }
  ]
}